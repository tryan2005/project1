"""
SMILE Fund SEC Data Pipeline
----------------------------
A monolithic ETL script to ingest, normalize, and model SEC EDGAR data for Power BI.

Core Capabilities:
- Hybrid Sync: Combines historical archives with incremental API updates.
- Star Schema: Produces DIM/FACT tables in Parquet format.
- Financial Logic: Handles TTM, Unit Scaling (Billions/Millions), Fiscal Alignments, and Weights.
- Discovery: Logs unknown XBRL tags for student review.

Author: University of Tennessee at Chattanooga - SMILE Fund
"""

import os
import json
import time
import shutil
import datetime
import logging
import argparse
import requests
import duckdb
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from pathlib import Path
from typing import Optional, Dict, Any

# ==============================================================================
# 1. CENTRAL CONFIGURATION
# ==============================================================================

CONFIG = {
    # Environment & Paths
    'base_dir': Path(os.getenv("SMILEFUND_BASE", r"C:\smilefund_project")),
    
    # Run Settings
    'start_date': "2015-01-01",
    'duck_memory': "24GB",  # Adjusted for 32GB RAM System
    'duck_threads': 8,
    
    # API / Network
    'sec_user_agent': "UTC SMILE Fund (vtn183@mocs.utc.edu)",
    'api_sleep': 0.15,
    'max_retries': 3,
    
    # Processing Logic
    'min_disk_gb': 5,
    'outlier_threshold': 1e16,
    'flush_buffer_size': 100_000,
}

# Directory Structure
DIRS = {
    'config': CONFIG['base_dir'] / "config",
    'raw_facts': CONFIG['base_dir'] / "data" / "sec" / "raw" / "company_facts",
    'raw_subm': CONFIG['base_dir'] / "data" / "sec" / "raw" / "submissions",
    'raw_meta': CONFIG['base_dir'] / "data" / "sec" / "raw" / "meta",
    'staging': CONFIG['base_dir'] / "warehouse" / "staging",
    'star': CONFIG['base_dir'] / "warehouse" / "star",
    'marts': CONFIG['base_dir'] / "warehouse" / "marts",
    'logs': CONFIG['base_dir'] / "warehouse" / "logs",
}

FILES = {
    'tag_map': DIRS['config'] / "xbrl_tag_map.csv",
    'tickers': DIRS['raw_meta'] / "company_tickers.json",
    'error_log': DIRS['logs'] / "etl_errors.log",
}

# Unit Normalization Logic (Base 1)
UNIT_SCALERS = {
    # Currency
    'USD': 1.0, 'usd': 1.0, 'iso4217:USD': 1.0,
    'USDThousands': 1000.0, 'usdthousands': 1000.0,
    'USDMillions': 1000000.0, 'usdmillions': 1000000.0,
    'USDBillions': 1000000000.0, 'usdbillions': 1000000000.0,

    # Shares
    'shares': 1.0,
    'sharesThousands': 1000.0, 'sharesthousands': 1000.0,
    'sharesMillions': 1000000.0, 'sharesmillions': 1000000.0,
    'sharesBillions': 1000000000.0, 'sharesbillions': 1000000000.0,

    # Ratios
    'pure': 1.0
}

# ==============================================================================
# 2. UTILITIES & INFRASTRUCTURE
# ==============================================================================

def setup_logging():
    """Initializes logging to file and console."""
    DIRS['logs'].mkdir(parents=True, exist_ok=True)
    logging.basicConfig(
        filename=FILES['error_log'],
        level=logging.INFO,
        format='%(asctime)s:%(levelname)s:%(message)s',
        filemode='a'
    )

def status(msg: str):
    print(f"[{datetime.datetime.now().strftime('%H:%M:%S')}] {msg}", flush=True)
    logging.info(msg)

class Timer:
    def __init__(self, label: str):
        self.label = label
        self.t0 = None
    def __enter__(self):
        self.t0 = time.time()
        status(f"▶ {self.label} — start")
        return self
    def __exit__(self, exc_type, exc, tb):
        dt = time.time() - self.t0
        if exc:
            status(f"✖ {self.label} — ERROR after {dt:,.1f}s: {exc}")
            logging.error(f"Phase {self.label} failed: {exc}", exc_info=True)
        else:
            status(f"✔ {self.label} — done in {dt:,.1f}s")

def get_duckdb():
    con = duckdb.connect()
    con.execute(f"PRAGMA threads={CONFIG['duck_threads']};") 
    con.execute(f"PRAGMA memory_limit='{CONFIG['duck_memory']}';")
    return con

def atomic_write_parquet(con, table_or_query: str, out_path: Path):
    """
    Writes data to a temporary file first, then renames it.
    This prevents Power BI lock contention and ensures partial writes don't corrupt history.
    """
    if not out_path.parent.exists():
        out_path.parent.mkdir(parents=True, exist_ok=True)

    temp_path = out_path.with_name(f"{out_path.stem}_temp{out_path.suffix}")
    
    # Detect if input is a query or a table name
    sql_clean = table_or_query.strip().upper()
    if sql_clean.startswith("SELECT") or sql_clean.startswith("WITH"):
        copy_cmd = f"COPY ({table_or_query}) TO '{temp_path}' (FORMAT PARQUET, COMPRESSION 'ZSTD')"
    else:
        copy_cmd = f"COPY {table_or_query} TO '{temp_path}' (FORMAT PARQUET, COMPRESSION 'ZSTD')"

    try:
        con.execute(copy_cmd)
        
        # Atomic Swap
        if out_path.exists():
            try:
                out_path.unlink()
            except PermissionError:
                status(f"⚠️ File Locked: {out_path.name} is open in another program. Data saved to {temp_path.name}")
                return 

        temp_path.rename(out_path)
        status(f"✔ Wrote {out_path.name}")
        
    except Exception as e:
        status(f"❌ Write Failed: {e}")
        logging.error(f"Atomic write failed for {out_path}", exc_info=True)
        if temp_path.exists(): temp_path.unlink()

def ensure_folders():
    for d in DIRS.values():
        d.mkdir(parents=True, exist_ok=True)
    
    # Create specific folders for every Star Schema table
    for sub in ["dim_company", "dim_calendar", "dim_metric", "dim_unit", 
                "dim_filing", "fact_financials", "dim_sector", "dim_listing", 
                "dim_security", "sic_sector_ranges"]:
        (DIRS['star'] / sub).mkdir(parents=True, exist_ok=True)

# ==============================================================================
# 3. API SYNCHRONIZATION
# ==============================================================================

def fetch_with_retry(url: str, headers: Dict) -> Optional[requests.Response]:
    """Robust HTTP fetcher with exponential backoff."""
    for attempt in range(CONFIG['max_retries']):
        try:
            r = requests.get(url, headers=headers, timeout=20)
            if r.status_code == 200:
                return r
            elif r.status_code == 429:
                sleep_time = (2 ** attempt) * 5
                status(f"API Rate Limit (429). Sleeping {sleep_time}s...")
                time.sleep(sleep_time)
            elif r.status_code in [500, 502, 503]:
                time.sleep(5)
            else:
                return r 
        except requests.exceptions.RequestException as e:
            logging.error(f"Network error {url}: {e}")
            time.sleep(5)
    return None

def update_from_api():
    status("Starting API Synchronization...")
    
    # Pre-flight Check
    total, used, free = shutil.disk_usage(CONFIG['base_dir'])
    if free // (2**30) < CONFIG['min_disk_gb']:
        status("❌ CRITICAL: Disk space low. Aborting API update.")
        return

    headers = {
        "User-Agent": CONFIG['sec_user_agent'],
        "Accept-Encoding": "gzip, deflate",
        "Host": "data.sec.gov"
    }

    # 1. Refresh Ticker Index
    r = fetch_with_retry("https://www.sec.gov/files/company_tickers.json", headers)
    if r and r.status_code == 200:
        with open(FILES['tickers'], "wb") as f: f.write(r.content)
        status("API: Ticker list updated.")
    else:
        status("API: Could not update tickers.")
        return

    # 2. Determine Active Universe
    with open(FILES['tickers'], "r") as f: 
        raw_tickers = json.load(f)
    active_ciks = [int(val['cik_str']) for val in raw_tickers.values()]
    status(f"API: Syncing {len(active_ciks)} active companies.")

    files_updated = 0
    errors = 0

    # 3. Incremental Download Loop
    for i, cik in enumerate(active_ciks):
        cik_padded = f"CIK{cik:010d}"
        
        # A. Company Facts (Financials)
        fact_path = DIRS['raw_facts'] / f"{cik_padded}.json"
        should_download = True
        
        # Check if file is fresh (less than 24h old)
        if fact_path.exists():
            mtime = datetime.datetime.fromtimestamp(fact_path.stat().st_mtime)
            if datetime.datetime.now() - mtime < datetime.timedelta(hours=24):
                should_download = False
        
        if should_download:
            url = f"https://data.sec.gov/api/xbrl/companyfacts/{cik_padded}.json"
            r = fetch_with_retry(url, headers)
            
            if r and r.status_code == 200:
                try:
                    json.loads(r.content) # JSON Integrity Check
                    tmp = fact_path.with_suffix(".tmp")
                    with open(tmp, "wb") as f: f.write(r.content)
                    tmp.replace(fact_path)
                    files_updated += 1
                except json.JSONDecodeError:
                    logging.error(f"Corrupt JSON download for {cik}")
            elif r and r.status_code != 404:
                errors += 1
            
            time.sleep(CONFIG['api_sleep'])

        # B. Submissions (Metadata/SIC)
        subm_path = DIRS['raw_subm'] / f"{cik_padded}.json"
        if not subm_path.exists() or should_download:
            url_sub = f"https://data.sec.gov/submissions/{cik_padded}.json"
            r_sub = fetch_with_retry(url_sub, headers)
            if r_sub and r_sub.status_code == 200:
                with open(subm_path, "wb") as f: f.write(r_sub.content)
            time.sleep(CONFIG['api_sleep'])

        if i % 100 == 0:
            status(f"API Progress: {i}/{len(active_ciks)} companies checked.")

    status(f"API Sync Complete. Updated {files_updated} files. Errors: {errors}")

# ==============================================================================
# 4. STAGING MODULE (JSON -> Parquet)
# ==============================================================================

def build_staging():
    if not FILES['tag_map'].exists():
        status("❌ CRITICAL: Configuration missing. Create xbrl_tag_map.csv.")
        return
    
    # Load Tag Governance
    tag_df = pd.read_csv(FILES['tag_map'])
    valid_tags = set(tag_df['xbrl_tag'].unique())
    tag_to_id = dict(zip(tag_df.xbrl_tag, tag_df.metric_id))
    
    unknown_tag_counter = {}

    schema = pa.schema([
        ("cik", pa.int64()), ("tag", pa.string()), ("metric_id", pa.int32()),
        ("val", pa.float64()), ("unit", pa.string()), 
        ("fy", pa.int32()), ("fp", pa.string()), ("form", pa.string()), 
        ("filed", pa.date32()), ("end", pa.date32()), ("frame", pa.string())
    ])

    out_file = DIRS['staging'] / "_best_local.parquet"
    writer = pq.ParquetWriter(out_file, schema, compression="zstd")
    rows_buffer = []
    
    files = list(DIRS['raw_facts'].glob("*.json"))
    status(f"Staging: Processing {len(files)} files...")

    for i, fpath in enumerate(files):
        try:
            with open(fpath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            cik = int(data.get('cik', 0))
            facts = data.get('facts', {}).get('us-gaap', {})

            for tag, content in facts.items():
                
                # Tag Governance & Discovery
                if tag not in valid_tags:
                    if 'units' in content:
                        unknown_tag_counter[tag] = unknown_tag_counter.get(tag, 0) + 1
                    continue

                mid = tag_to_id.get(tag)

                for unit_name, records in content.get('units', {}).items():
                    # Unit Scaling (Billions/Millions/Thousands)
                    scale_factor = 1.0
                    for k, v in UNIT_SCALERS.items():
                        if k in unit_name:
                            scale_factor = v
                            break
                    
                    for r in records:
                        # Safety Checks
                        try:
                            filed_dt = pd.to_datetime(r.get('filed'), errors='coerce')
                            end_dt = pd.to_datetime(r.get('end'), errors='coerce')
                            
                            # Skip invalid dates or data before 2015
                            if pd.isna(filed_dt) or pd.isna(end_dt): continue
                            if str(end_dt.date()) < CONFIG['start_date']: continue
                        except: continue
                        
                        # Value Normalization
                        try:
                            raw_val = float(r['val'])
                            final_val = raw_val * scale_factor
                            # Outlier Protection
                            if abs(final_val) > CONFIG['outlier_threshold']:
                                continue
                        except: continue

                        rows_buffer.append({
                            'cik': cik, 'tag': tag, 'metric_id': mid,
                            'val': final_val, 'unit': unit_name,
                            'fy': r.get('fy', 0), 'fp': r.get('fp', ''),
                            'form': r.get('form', ''),
                            'filed': filed_dt.date(),
                            'end': end_dt.date(),
                            'frame': r.get('frame', None)
                        })

        except json.JSONDecodeError:
            logging.error(f"Corrupt JSON: {fpath.name}")
        except Exception as e:
            logging.error(f"Staging Error {fpath.name}: {e}")

        # Memory Management
        if len(rows_buffer) > CONFIG['flush_buffer_size']:
            writer.write_table(pa.Table.from_pylist(rows_buffer, schema=schema))
            rows_buffer = []
            if i % 1000 == 0: status(f"Staging: Scanned {i} files...")

    if rows_buffer:
        writer.write_table(pa.Table.from_pylist(rows_buffer, schema=schema))
    writer.close()

    # Reporting
    if unknown_tag_counter:
        df_missing = pd.DataFrame(list(unknown_tag_counter.items()), columns=['xbrl_tag', 'frequency'])
        df_missing.sort_values(by='frequency', ascending=False).to_csv(DIRS['logs'] / "missing_tags_report.csv", index=False)

    # Deduplication (Best Record Strategy)
    con = get_duckdb()
    dedup_query = f"""
        SELECT * EXCLUDE (rn) FROM (
            SELECT *, ROW_NUMBER() OVER (
                PARTITION BY cik, metric_id, fy, fp, "end" 
                ORDER BY 
                    CASE WHEN frame IS NOT NULL THEN 1 ELSE 0 END DESC,
                    CASE WHEN form LIKE '10-K%%' THEN 1 ELSE 0 END DESC,
                    filed DESC 
            ) as rn
            FROM '{str(out_file)}'
        ) WHERE rn = 1
    """
    atomic_write_parquet(con, dedup_query, out_file)
    con.close()
    status("Staging: Complete.")

# ==============================================================================
# 5. DIMENSION MODULE (Star Schema)
# ==============================================================================

def build_dimensions():
    con = get_duckdb()
    ts = datetime.datetime.now().strftime("%Y%m%d")

    # 1. Dim Sector
    sector_data = [
        (10, 'Energy'), (15, 'Materials'), (20, 'Industrials'),
        (25, 'Consumer Discretionary'), (30, 'Consumer Staples'), (35, 'Health Care'),
        (40, 'Financials'), (45, 'Information Technology'), (50, 'Communication Services'),
        (55, 'Utilities'), (60, 'Real Estate'), (99, 'Unmapped')
    ]
    con.execute("CREATE OR REPLACE TABLE dim_sector (sector_id INT, sector_name VARCHAR)")
    con.executemany("INSERT INTO dim_sector VALUES (?, ?)", sector_data)
    atomic_write_parquet(con, "dim_sector", DIRS['star'] / "dim_sector/dim_sector.parquet")

    # 2. Dim Metric (From Config)
    atomic_write_parquet(con, 
        f"SELECT DISTINCT metric_id, canonical_metric as metric_name, xbrl_tag, report_section FROM read_csv_auto('{FILES['tag_map']}')", 
        DIRS['star'] / f"dim_metric/dim_metric_{ts}.parquet"
    )

    # 3. Dim Unit (Dynamic)
    atomic_write_parquet(con,
        f"SELECT row_number() OVER () as unit_id, unit as unit_code FROM (SELECT DISTINCT unit FROM '{DIRS['staging']}/_best_local.parquet')",
        DIRS['star'] / f"dim_unit/dim_unit_{ts}.parquet"
    )

    # 4. Dim Company (Tickers + SIC Logic + Submissions Fallback)
    con.execute("""
        CREATE TEMP TABLE sic_ranges (sector_id INT, min_sic INT, max_sic INT);
        INSERT INTO sic_ranges VALUES 
        (10, 1000, 1499), (10, 2900, 2999), (15, 2000, 2099), (15, 2500, 3999),
        (20, 1500, 1799), (20, 4000, 4299), (20, 4500, 4799), (20, 5000, 5199),
        (25, 5600, 5999), (25, 7000, 7999), (30, 5200, 5599), (35, 2830, 2839),
        (35, 8000, 8099), (40, 6000, 6499), (40, 6700, 6799), (45, 3570, 3679),
        (45, 7370, 7379), (50, 4800, 4899), (55, 4900, 4999), (60, 6500, 6699);
    """)

    has_tickers = False
    if FILES['tickers'].exists():
        with open(FILES['tickers'], 'r') as f: tdata = json.load(f)
        df_tickers = pd.DataFrame.from_dict(tdata, orient='index')
        con.register('tickers_raw', df_tickers)
        has_tickers = True

    # Identify Universe from Staging Data
    query = f"WITH unique_ciks AS (SELECT DISTINCT cik FROM '{DIRS['staging']}/_best_local.parquet')"
    
    if has_tickers:
        query += """
        , base_companies AS (
            SELECT 
                u.cik::BIGINT as company_id,
                u.cik::BIGINT as cik,
                COALESCE(t.title, 'Delisted/Unknown') as name,
                t.ticker,
                t.sic_code::INT as sic_code
            FROM unique_ciks u
            LEFT JOIN tickers_raw t ON u.cik = t.cik
        )
        """
    else:
        query += ", base_companies AS (SELECT u.cik as company_id, u.cik, 'Unknown' as name, NULL as ticker, NULL as sic_code FROM unique_ciks u)"

    query += """
        SELECT b.*, COALESCE(s.sector_id, 99) as sector_id 
        FROM base_companies b
        LEFT JOIN sic_ranges s ON b.sic_code BETWEEN s.min_sic AND s.max_sic
    """
    
    atomic_write_parquet(con, query, DIRS['star'] / f"dim_company/dim_company_{ts}.parquet")

    # 5. Dim Calendar
    atomic_write_parquet(con, f"""
        WITH dates AS (
            SELECT DISTINCT "end" as dt FROM '{DIRS['staging']}/_best_local.parquet'
            UNION SELECT DISTINCT filed as dt FROM '{DIRS['staging']}/_best_local.parquet'
        )
        SELECT 
            strftime(dt, '%Y%m%d')::INT as calendar_id,
            dt as date,
            year(dt) as cal_year,
            quarter(dt) as cal_quarter,
            month(dt) as cal_month,
            'calendar' as period_type
        FROM dates WHERE dt IS NOT NULL
    """, DIRS['star'] / f"dim_calendar/dim_calendar_{ts}.parquet")
    
    con.close()
    status("Dimensions: Built.")

# ==============================================================================
# 6. FACT MODULE
# ==============================================================================

def build_facts():
    con = get_duckdb()
    ts = datetime.datetime.now().strftime("%Y%m%d")
    
    if not (DIRS['staging'] / "_best_local.parquet").exists(): return

    tag_df = pd.read_csv(FILES['tag_map'])
    con.register('tag_map', tag_df)
    
    # Load Dim Unit to resolve IDs
    con.execute(f"CREATE VIEW v_unit AS SELECT * FROM read_parquet('{DIRS['star']}/dim_unit/*.parquet')")

    # Join Staging -> Tag Map (Weights) -> Dim Unit (IDs)
    query = f"""
        SELECT 
            s.cik as company_id,
            s.metric_id,
            strftime(s."end", '%Y%m%d')::INT as calendar_id,
            (s.val * COALESCE(tm.weight, 1)) as value,
            COALESCE(u.unit_id, 1) as unit_id,
            s.fp as fiscal_period, 
            s.fy as fiscal_year,
            s.form,
            'CONSOLIDATED' as consolidated_flag
        FROM '{DIRS['staging']}/_best_local.parquet' s
        JOIN tag_map tm ON s.metric_id = tm.metric_id
        LEFT JOIN v_unit u ON s.unit = u.unit_code
    """

    atomic_write_parquet(con, query, DIRS['star'] / "fact_financials" / f"fact_financials_{ts}.parquet")
    con.close()
    status("Facts: Built.")

# ==============================================================================
# 7. MARTS MODULE (Analytics)
# ==============================================================================

def build_marts():
    con = get_duckdb()
    
    try:
        con.execute(f"CREATE VIEW v_fact AS SELECT * FROM read_parquet('{DIRS['star']}/fact_financials/*.parquet')")
        con.execute(f"CREATE VIEW v_comp AS SELECT * FROM read_parquet('{DIRS['star']}/dim_company/*.parquet')")
        con.execute(f"CREATE VIEW v_metr AS SELECT * FROM read_parquet('{DIRS['star']}/dim_metric/*.parquet')")
        con.execute(f"CREATE VIEW v_cal  AS SELECT * FROM read_parquet('{DIRS['star']}/dim_calendar/*.parquet')")
    except Exception as e:
        status(f"Marts skipped: Star schema missing. {e}")
        return

    # Base View: Calculates Fiscal Sequence for Sorting
    con.execute("""
        CREATE TEMP TABLE base_financials AS
        SELECT 
            f.company_id, c.ticker, c.sector_id,
            m.metric_name, m.report_section,
            cl.date as report_date, 
            COALESCE(f.fiscal_year, cl.cal_year) as fiscal_year,
            COALESCE(f.fiscal_period, CAST(cl.cal_quarter AS VARCHAR)) as fiscal_period,
            
            -- Sort Key: (Year * 4) + Quarter Index
            (COALESCE(f.fiscal_year, cl.cal_year) * 4) + 
            CASE 
                WHEN f.fiscal_period = 'Q1' THEN 1
                WHEN f.fiscal_period = 'Q2' THEN 2
                WHEN f.fiscal_period = 'Q3' THEN 3
                WHEN f.fiscal_period = 'Q4' THEN 4
                WHEN f.fiscal_period = 'FY' THEN 4
                ELSE cl.cal_quarter 
            END as fiscal_sequence,
            
            f.value
        FROM v_fact f
        JOIN v_comp c ON f.company_id = c.company_id
        JOIN v_metr m ON f.metric_id = m.metric_id
        JOIN v_cal cl ON f.calendar_id = cl.calendar_id
    """)

    # --- MART 1: QUARTERLY ---
    atomic_write_parquet(con, "base_financials", DIRS['marts'] / "mart_quarterly.parquet")

    # --- MART 2: TTM ---
    con.execute("""
        WITH windowed AS (
            SELECT 
                *,
                SUM(value) OVER (
                    PARTITION BY company_id, metric_name 
                    ORDER BY fiscal_sequence 
                    ROWS BETWEEN 3 PRECEDING AND CURRENT ROW
                ) as sum_4q,
                COUNT(*) OVER (
                    PARTITION BY company_id, metric_name 
                    ORDER BY fiscal_sequence 
                    ROWS BETWEEN 3 PRECEDING AND CURRENT ROW
                ) as count_4q
            FROM base_financials
        )
        SELECT 
            company_id, ticker, metric_name, report_date, fiscal_year, fiscal_period,
            CASE 
                WHEN report_section = 'Balance Sheet' THEN value
                WHEN count_4q < 4 THEN NULL 
                ELSE sum_4q 
            END as value_ttm
        FROM windowed
    """)

    atomic_write_parquet(con, "mart_ttm", DIRS['marts'] / "mart_ttm.parquet")
    con.close()
    status("Marts: Built.")

# ==============================================================================
# 8. MAIN
# ==============================================================================

def main():
    parser = argparse.ArgumentParser(description="SMILE Fund ETL Pipeline")
    parser.add_argument('--full', action='store_true', help="Run API Sync + ETL")
    parser.add_argument('--etl-only', action='store_true', help="Run ETL on local data only")
    parser.add_argument('--api-only', action='store_true', help="Run API Sync only")
    args = parser.parse_args()

    # Interactive Mode Fallback
    if not any(vars(args).values()):
        print("No arguments provided. Select Mode:")
        print("1. Full Update (API Sync + ETL)")
        print("2. ETL Only (Local Data)")
        choice = input("Enter 1 or 2: ")
        if choice == '1': args.full = True
        else: args.etl_only = True

    ensure_folders()
    setup_logging()

    status("Initializing Pipeline...")

    if args.full or args.api_only:
        with Timer("API Sync"):
            update_from_api()

    if args.full or args.etl_only:
        with Timer("Total ETL"):
            with Timer("1. Staging"): build_staging()
            with Timer("2. Dimensions"): build_dimensions()
            with Timer("3. Facts"): build_facts()
            with Timer("4. Marts"): build_marts()
    
    print("\n✅ Pipeline Complete.")

if __name__ == "__main__":
    main()

# build_all_marts_v2.py                                         # Script name (for reference)
# SEC company_facts (USD) → ALL metrics → Quarters (true) → TTM → Calendar alignment   # High-level pipeline description
# OUTPUTS (no deletes; writes to marts_v2/):                    # Where outputs are written (no files deleted)
#   warehouse/parquet/marts_v2/                                 # Root output directory for marts
#     fact_quarters_true.parquet                                 # Quarters-by-quarter facts (Q1–Q3 from 10-Q, Q4 via 10-K delta)
#     fact_ttm_aligned.parquet                                   # TTM values aligned to calendar year/quarter
#     dim_security.parquet                                       # Security/company dimension (CIK, name, ticker)
#     vw_latest_ttm.parquet                                      # View of most recent TTM per (cik, tag) with ticker
#     dim_fiscal_year_end.parquet                                # Inferred fiscal year-end month by CIK
#     _logs/*                                                    # Log area (currently not written to by this script)

import os, json                                                  # Standard libs: os (filesystem helpers), json (parse SEC files)
from pathlib import Path                                         # Path for path-safe filesystem operations
from collections import Counter                                  # Counter used to pick most-frequent month for FYE inference
from typing import Dict, Iterable, Tuple                         # Type hints for clarity
import pandas as pd                                              # Pandas for dataframes and data wrangling
# --------- CONFIG ---------
BASE = Path(r"C:\smilefund_project")                             # Project root directory (Windows raw string)
FACTS = BASE / r"data\sec\company_facts"                         # Folder containing SEC company_facts JSON files
DATA  = BASE / r"data\sec"                                       # General SEC data folder (for ticker_map.csv, etc.)
WARE  = BASE / r"warehouse\parquet"                              # Warehouse/parquet root directory
MARTS_V2_NAME = "marts_v2"                                       # Name of the subfolder for this version of marts
MARTS = WARE / MARTS_V2_NAME                                     # Full output path for marts_v2
LOGS  = MARTS / "_logs"                                          # Logs subdirectory (reserved)
MARTS.mkdir(parents=True, exist_ok=True)                         # Create marts_v2 directory (no error if it already exists)
LOGS.mkdir(parents=True, exist_ok=True)                          # Create logs directory (no error if it already exists)
# --------- CONSTANTS ---------
USD_UNITS_MULT: Dict[str,int] = {                                # Mapping of SEC unit strings → numeric multipliers
    "USD":1, "USD$":1,                                           # Unit variants for plain dollars
    "USDthousands":1_000, "USDThousands":1_000,                  # Unit variants for thousands
    "USDm":1_000_000, "USDmillions":1_000_000, "USDMillions":1_000_000,  # Unit variants for millions
}
Q_MIN, Q_MAX = 70, 110                                           # Acceptable 10-Q duration range in days (approx quarter)
FY_MIN, FY_MAX = 330, 380                                        # Acceptable 10-K duration range in days (approx fiscal year)
def parse_dt(s): return pd.to_datetime(s, format="%Y-%m-%d", errors="coerce")  # Parse YYYY-MM-DD → Timestamp (NaT on failure)
def duration_days(start,end):                                    # (Deprecated stub; kept for clarity comment below)
    s,e = parse_dt(start), parse_dt(end)                         # Parse start/end strings to timestamps
    return int((e-e).days)+1 if (pd.notna(s) and pd.notna(e)) else None  # BUGGY: always zero; fixed in function below
# ----- helpers (fixed) -----
def duration_days(start, end):                                   # Compute inclusive day-count between start and end dates
    s, e = parse_dt(start), parse_dt(end)                        # Parse both dates
    if pd.isna(s) or pd.isna(e): return None                     # If either missing/unparseable → None
    return int((e - s).days) + 1                                 # Inclusive difference in days
def is_duration(rec: dict) -> bool:                              # Determine if SEC record is "duration" (has start date)
    return rec.get("start") is not None                          # Presence of "start" implies duration; otherwise instant
def month_from_date(s: str):                                     # Extract month (1–12) from "YYYY-MM-DD", else None
    try:
        return int(s[5:7]) if s and len(s) >= 7 else None        # Slice characters 5–6 (0-based) if string is long enough
    except:
        return None                                              # Any failure → None
def qn_from_fp(fp: str):                                         # Map SEC fp (Q1,Q2,Q3,Q4,10K delta) → quarter number 1–4
    return {"Q1":1,"Q2":2,"Q3":3,"Q4":4,"10K delta":4}.get(fp)   # Q4 and 10K delta both map to quarter 4
def quarter_end_month_from_qn(qn: int, fye_month: int) -> int:   # Quarter end month given quarter number and FYE month
    return ((int(fye_month) + {4:0,3:-3,2:-6,1:-9}[int(qn)] - 1) % 12) + 1  # Roll back months from FYE to get quarter end
def calendar_year_from_fy(fy: int, q_end_month: int, fye_month: int) -> int: # Convert FY/quarter-end-month → calendar year
    return int(fy) if int(q_end_month) <= int(fye_month) else int(fy) - 1    # If quarter end before/equal FYE month, same FY; else prior
# --------- READERS ---------
def load_company_file(path: Path) -> Tuple[int, dict]:           # Load a single company_facts JSON file
    d = json.loads(path.read_text(encoding="utf-8"))             # Read and parse JSON
    return int(d.get("cik", 0)), d                               # Return (cik, full dict)
def iter_all_us_gaap_usd(cik: int, data: dict):                  # Yield all USD us-gaap records for a given company
    facts = data.get("facts", {}).get("us-gaap", {})             # Dive into facts → us-gaap tag dictionary
    for tag, blk in facts.items():                                # Iterate each XBRL tag and its block
        for unit, arr in blk.get("units", {}).items():           # Iterate each unit (e.g., USD, USDm) and its array of records
            if unit not in USD_UNITS_MULT: continue              # Only keep USD-based units we know how to scale
            mult = USD_UNITS_MULT[unit]                          # Get multiplier for this unit
            for rec in arr:                                      # Iterate every reported record for this (tag, unit)
                fp, form, fy, val = rec.get("fp"), rec.get("form"), rec.get("fy"), rec.get("val")  # Extract key fields
                filed, start, end = rec.get("filed"), rec.get("start"), rec.get("end")             # Extract period metadata
                if val is None or fy is None or fp is None or form is None:                        # Filter incomplete rows
                    continue
                pt = "duration" if is_duration(rec) else "instant"                                  # Period type: duration vs instant
                if pt == "duration":                                                                # Duration records (have start/end)
                    d = duration_days(start, end)                                                   # Compute length in days
                    if fp in ("Q1","Q2","Q3"):                                                      # For Q1–Q3…
                        if not (form.startswith("10-Q") and d and Q_MIN <= d <= Q_MAX): continue    # Keep only proper 10-Q duration
                    elif fp == "FY":                                                                # For FY (annual)…
                        if not (form.startswith("10-K") and d and FY_MIN <= d <= FY_MAX): continue  # Keep only proper 10-K duration
                    elif fp == "Q4":                                                                # We do not accept Q4 duration directly
                        continue  # we’ll derive Q4 via 10-K delta                                   # Q4 derived as FY − (Q1+Q2+Q3)
                else:                                                                               # Instant records (balance sheet, etc.)
                    if fp not in ("Q1","Q2","Q3","Q4","FY"): continue                               # Only keep quarter/fiscal endpoints

                try: value = float(val) * mult                                                      # Scale numeric value by unit multiplier
                except: continue                                                                    # Skip if value is non-numeric
                yield {                                                                             # Yield normalized record
                    "cik": cik, "tag": tag, "unit": unit,                                           # Entity and unit
                    "fy": int(fy), "fp": str(fp), "form": str(form),                                # Fiscal year, period, form
                    "filed": str(filed) if filed else None,                                         # Filed date as string (or None)
                    "start": start, "end": end,                                                     # Period bounds (None for instant)
                    "period_type": pt,                                                              # "duration" or "instant"
                    "value": value,                                                                 # Scaled numeric value
                }
def choose_best_per_period(df: pd.DataFrame) -> pd.DataFrame:    # Deduplicate competing filings; pick best by form & recency
    df = df.copy()                                               # Work on a copy to avoid side effects
    df["filed_dt"] = pd.to_datetime(df["filed"], errors="coerce")# Parse filed date for sorting
    def form_rank(r):                                            # Assign rank: prefer 10-Q for quarters, 10-K for FY
        pt, fp, form = r["period_type"], r["fp"], r["form"]      # Pull needed fields
        if pt == "duration":
            if fp in ("Q1","Q2","Q3"): return 0 if form.startswith("10-Q") else 1  # 10-Q preferred for Q1–Q3
            if fp == "FY":             return 0 if form.startswith("10-K") else 1  # 10-K preferred for FY
            return 1
        else:
            if fp in ("Q1","Q2","Q3","Q4"): return 0 if form.startswith("10-Q") else 1  # 10-Q preferred for quarters
            if fp == "FY":                  return 0 if form.startswith("10-K") else 1  # 10-K preferred for FY
            return 1
    df["form_rank"] = df.apply(form_rank, axis=1)                # Compute preferred form rank per row
    df = df.sort_values(                                         # Sort by entity/tag/period, then pref, then filed date
        ["cik","tag","fy","fp","period_type","form_rank","filed_dt"],
        ascending=[True,True,True,True,True,True,True]
    )
    return df.drop_duplicates(["cik","tag","fy","fp","period_type"], keep="first") \  # Keep the top-ranked row per key
             .drop(columns=["form_rank"])                                             # Drop helper column
# --------- QUARTERS (TRUE) ---------
def build_quarters_true(best: pd.DataFrame) -> pd.DataFrame:     # Construct true quarter series: Q1–Q3 from 10-Q; Q4 via 10-K delta
    out = []                                                     # Collector for partial dataframes
    # duration: Q1..Q3 from 10-Q; Q4 as 10-K delta
    dur = best[best["period_type"]=="duration"]                  # Filter duration-type facts
    if not dur.empty:                                            # If we have any duration rows
        q = dur[dur["fp"].isin(["Q1","Q2","Q3"])][["cik","tag","fy","fp","value"]]     # Keep Q1–Q3 as-is
        fy = dur[dur["fp"]=="FY"][["cik","tag","fy","value"]].rename(columns={"value":"fy_value"})  # FY totals per tag
        qsum = (q.groupby(["cik","tag","fy"], as_index=False)["value"].sum()            # Sum Q1–Q3 values
                  .rename(columns={"value":"q123"}))                                    # Name the sum column
        m = fy.merge(qsum, on=["cik","tag","fy"], how="inner")                          # Join FY with Q1–Q3 sums
        m["delta"] = m["fy_value"] - m["q123"]                                          # Q4 (delta) = FY − (Q1+Q2+Q3)
        out.append(q.assign(period_type="duration"))                                    # Append Q1–Q3 duration rows
        out.append(m[["cik","tag","fy","delta"]].rename(columns={"delta":"value"})      # Append Q4 as "10K delta"
                   .assign(fp="10K delta", period_type="duration"))
    # instant: Q1..Q4 (prefer 10-Q). If only FY exists, map FY→Q4 (for that year only)
    ins = best[best["period_type"]=="instant"]                    # Filter instant-type facts
    if not ins.empty:                                             # If any instant rows
        iq = ins[ins["fp"].isin(["Q1","Q2","Q3","Q4"])][["cik","tag","fy","fp","value"]]  # Keep quarterly instants
        out.append(iq.assign(period_type="instant"))              # Append them
        fy_ins = ins[ins["fp"]=="FY"][["cik","tag","fy","value"]] # FY instant values (some tags report FY instant)
        if not fy_ins.empty:                                      # If we have FY instants
            have_q4 = set(iq[iq["fp"]=="Q4"][["cik","tag","fy"]].apply(tuple, axis=1))   # Build set of (cik,tag,fy) that already have Q4
            map_df = fy_ins[~fy_ins.apply(lambda r: (r["cik"],r["tag"],r["fy"]) in have_q4, axis=1)]  # FY rows lacking a Q4
            if not map_df.empty:
                out.append(map_df.assign(fp="Q4", period_type="instant"))  # Treat FY instant as Q4 if no Q4 present
    if not out:                                                   # If we collected nothing…
        return pd.DataFrame(columns=["cik","tag","fy","fp","qn","value","period_type"])  # Return empty with expected columns
    rows = pd.concat(out, ignore_index=True)                      # Combine pieces into a single dataframe
    rows["qn"] = rows["fp"].map(qn_from_fp).astype("Int64")       # Map fp → quarter number (nullable integer)
    rows = rows.dropna(subset=["qn"]).copy()                      # Drop rows without a valid quarter number
    return rows[["cik","tag","fy","fp","qn","value","period_type"]] \  # Return tidy columns
              .sort_values(["cik","tag","fy","qn"])               # Sort by entity/tag/year/quarter
# --------- TTM ---------
def build_ttm(qu: pd.DataFrame) -> pd.DataFrame:                  # Compute trailing-twelve-month values per tag/entity
    if qu.empty: return pd.DataFrame()                            # If no quarters available, return empty
    qu = qu.sort_values(["cik","tag","fy","qn"]).copy()           # Ensure chronological ordering within groups
    def per_group(g):                                             # Apply per (cik, tag, period_type) group
        g = g.sort_values(["fy","qn"])                            # Sort by fiscal year then quarter
        if g["period_type"].iloc[0] == "duration":                # For duration series…
            g["TTM_value"] = g["value"].rolling(4, min_periods=4).sum()  # TTM = rolling 4-quarter sum
        else:                                                     # For instant series…
            g["TTM_value"] = g["value"]                           # TTM identity (common for stock measures)
        return g
    ttm = qu.groupby(["cik","tag","period_type"], group_keys=False).apply(per_group)  # Compute TTM per group
    return ttm.dropna(subset=["TTM_value"])                       # Keep rows where a full 4-quarter TTM exists
# --------- FYE & alignment ---------
def infer_fye_months() -> pd.DataFrame:                           # Infer each company's fiscal year-end month from FY records
    files = list(FACTS.glob("*.json"))                            # Enumerate all company_facts JSON files
    out = []                                                      # Collector for (cik, fye_month)
    for i, p in enumerate(files, 1):                              # Iterate with progress index
        try:
            _, d = load_company_file(p)                           # Load JSON dict
            facts = d.get("facts", {}).get("us-gaap", {})         # Navigate to us-gaap tags
            months = []                                           # Collector for FY end months seen in records
            for _, blk in facts.items():                          # For each tag block
                for _, arr in blk.get("units", {}).items():       # For each unit's record array
                    for rec in arr:                               # For each record
                        if rec.get("fp") == "FY":                 # Only FY rows (annual)
                            m = month_from_date(rec.get("end"))   # Extract month from period end date
                            if m: months.append(m)                # Collect month if valid
            if months:                                            # If any months were found
                from collections import Counter                   # (Local import okay; already imported above)
                c = Counter(months)                               # Count frequency of months
                out.append((int(d.get("cik",0)), max(c.items(), key=lambda kv: (kv[1], kv[0]))[0]))  # Pick most common (tie → larger month)
        except: pass                                              # Ignore any file/parse errors and continue
        if i % 2000 == 0:                                         # Progress heartbeat every 2000 companies
            print(f"inferred FYE for {i}/{len(files)} companies")
    dim = pd.DataFrame(out, columns=["cik","fye_month"]).drop_duplicates("cik")  # Build dimension with unique CIK rows
    dim["fiscal_year_start_month"] = ((dim["fye_month"].fillna(12).astype("Int64") % 12) + 1).astype("Int64")  # Derive start month
    dim.to_parquet(MARTS / "dim_fiscal_year_end.parquet")         # Persist the FYE dimension
    return dim                                                    # Return dataframe for downstream joins
def align_to_calendar(ttm: pd.DataFrame, dim_fye: pd.DataFrame) -> pd.DataFrame:  # Map TTM rows to calendar year/quarter
    a = ttm.merge(dim_fye, on="cik", how="left")                  # Join FYE month onto TTM rows by CIK
    mask = a["fye_month"].notna() & a["qn"].notna()               # Only rows with known FYE and quarter number can be aligned
    end_m, cal_y, cal_q = [], [], []                              # Buffers for computed quarter end month/year/quarter
    for fy, qn, fye in a.loc[mask, ["fy","qn","fye_month"]].itertuples(index=False):  # Iterate needed fields for alignment
        em = quarter_end_month_from_qn(int(qn), int(fye))         # Compute quarter end month for this qn/fye
        end_m.append(em)                                          # Collect end month
        cal_y.append(calendar_year_from_fy(int(fy), em, int(fye)))# Compute calendar year associated with this quarter
        cal_q.append((em-1)//3 + 1)                               # Convert month → calendar quarter 1–4
    idx = a.index[mask]                                           # Index positions where we computed alignment
    a.loc[idx, "quarter_end_month"] = end_m                       # Write quarter end month
    a.loc[idx, "calendar_year"]     = cal_y                       # Write calendar year
    a.loc[idx, "calendar_quarter"]  = cal_q                       # Write calendar quarter
    return a                                                      # Return aligned dataframe
# --------- dim_security + ticker map ---------
def build_dim_security() -> pd.DataFrame:                         # Build security/company dimension from company_facts + optional map
    rows = []                                                     # Collect raw rows
    files = list(FACTS.glob("*.json"))                            # Enumerate all company facts files
    for i, p in enumerate(files, 1):                              # Iterate with progress
        try:
            d = json.loads(p.read_text(encoding="utf-8"))         # Read file content and parse JSON
            cik = int(d.get("cik", 0))                            # Extract CIK
            name = d.get("entityName") or ""                      # Extract company/entity name
            ticker = d.get("ticker") or d.get("tickers")          # Extract ticker (single or list)
            if isinstance(ticker, list) and ticker: ticker = str(ticker[0])  # Normalize to a single string if list
            rows.append({"cik": cik, "company_name": name, "ticker": ticker})  # Append base row
        except: pass                                              # Ignore malformed files
        if i % 2000 == 0: print(f"dim_security progress {i}/{len(files)}")     # Progress heartbeat
    dim = pd.DataFrame(rows).drop_duplicates("cik")               # Deduplicate by CIK
    dim["cik"] = pd.to_numeric(dim["cik"], errors="coerce").astype("Int64")    # Ensure nullable integer CIK type
    dim["ticker"] = (dim["ticker"].astype(str).str.upper().str.strip())        # Uppercase/trim tickers
    dim.loc[dim["ticker"].isin(["NONE","NAN","<NA>",""]), "ticker"] = pd.NA    # Normalize invalid tickers to NA
    dim["company_name"] = (dim["company_name"].astype(str)                      # Clean up company names
                           .str.replace(r"[^A-Za-z0-9 &\.-]+"," ",regex=True)
                           .str.replace(r"\s+"," ",regex=True)
                           .str.strip())
    # merge your ticker_map.csv if present
    map_csv = DATA / "ticker_map.csv"                              # Expected path of user-provided CIK→ticker map
    if map_csv.exists():                                           # If a map file exists…
        tm = pd.read_csv(map_csv)                                  # Read the CSV
        tm.columns = [c.strip().lower() for c in tm.columns]       # Normalize column names (lowercase, trimmed)
        if "cik" in tm.columns and "ticker" in tm.columns:         # Require columns: cik, ticker
            tm["cik"] = pd.to_numeric(tm["cik"], errors="coerce").astype("Int64")        # Normalize CIK type
            tm["ticker"] = (tm["ticker"].astype(str).str.upper().str.strip()             # Normalize tickers
                            .replace("-", ".", regex=False))                              # Map '-' to '.' (ticker convention)
            tm = tm.dropna(subset=["cik"]).drop_duplicates("cik", keep="first")          # One row per CIK
            dim = dim.merge(tm[["cik","ticker"]], on="cik", how="left", suffixes=("","_map"))  # Left-join map onto base
            dim["ticker"] = dim["ticker_map"].combine_first(dim["ticker"])               # Prefer mapped ticker if present
            dim.drop(columns=["ticker_map"], inplace=True, errors="ignore")               # Remove helper column
            print(f"merged ticker_map.csv with {len(tm):,} rows")                      # Log merge result
        else:
            print("ticker_map.csv missing 'cik' or 'ticker' — skipped merge")         # Warn about bad schema
    else:
        print("ticker_map.csv not found — dim_security uses embedded tickers only")    # Inform if no map present
    dim.to_parquet(MARTS / "dim_security.parquet")                 # Persist dim_security
    return dim                                                     # Return for downstream joins/views
# --------- MAIN ---------
def main():                                                       # Orchestrates the full build pipeline
    files = list(FACTS.glob("*.json"))                            # Enumerate input company_facts files
    if not files:                                                 # If none found…
        raise RuntimeError(f"No company_facts JSON files in {FACTS}")  # Hard fail with message
    # A) parse all USD us-gaap rows
    raw_rows = []                                                 # Collector for normalized raw records
    for i, p in enumerate(files, 1):                              # Iterate files with progress index
        try:
            cik, data = load_company_file(p)                      # Load (cik, dict)
            raw_rows.extend(iter_all_us_gaap_usd(cik, data))      # Append all eligible records from this company
        except: pass                                              # Ignore individual file errors to keep going
        if i % 2000 == 0: print(f"parsed {i}/{len(files)} companies")  # Progress heartbeat
    raw = pd.DataFrame(raw_rows)                                  # Build dataframe from collected records
    if raw.empty:                                                 # If nothing parsed…
        raise RuntimeError("No USD us-gaap facts parsed.")        # Fail fast—likely wrong input path or data
    # B) choose “best” per (cik, tag, fy, fp, period_type)
    best = choose_best_per_period(raw)                            # Deduplicate by picking preferred filing for each period
    # C) quarters (true)
    qu = build_quarters_true(best)                                # Build true quarters (incl. Q4 from 10-K delta)
    (MARTS / "fact_quarters_true.parquet").write_bytes(b"") if False else None  # No-op placeholder (kept from prior pattern)
    qu.to_parquet(MARTS / "fact_quarters_true.parquet")           # Persist quarters table
    # D) TTM
    ttm = build_ttm(qu)                                           # Compute TTM values over quarter series
    if ttm.empty:                                                 # If no TTM could be formed…
        raise RuntimeError("No TTM rows produced.")               # Fail (upstream data likely insufficient)
    # E) FYE → calendar alignment
    dim_fye = infer_fye_months()                                  # Infer FYE month per company (and persist)
    ttm_aligned = align_to_calendar(ttm, dim_fye)                 # Map TTM rows to calendar year/quarter
    ttm_aligned.to_parquet(MARTS / "fact_ttm_aligned.parquet")    # Persist aligned TTM
    # F) dim_security (+ ticker_map merge)
    dim_sec = build_dim_security()                                # Build and persist security dimension (with optional map)
    # G) latest TTM per (cik, tag) + ticker
    a = ttm_aligned.dropna(subset=["calendar_year","qn"]).copy()  # Keep rows with valid calendar alignment
    a["calendar_year"] = a["calendar_year"].astype(int)           # Normalize calendar year to int
    a["qn"] = a["qn"].astype(int)                                 # Normalize quarter number to int
    last = (a.sort_values(["cik","tag","calendar_year","qn"])     # Sort so that group .last() picks most recent quarter
              .groupby(["cik","tag"], as_index=False)             # Group by company & tag
              .last()[["cik","tag","calendar_year","calendar_quarter","period_type","TTM_value"]]  # Take last row per group
              .rename(columns={"calendar_year":"ttm_cal_year",    # Rename columns for clarity in view
                               "calendar_quarter":"ttm_cal_quarter",
                               "TTM_value":"ttm_value"}))
    vw = (last.merge(dim_sec, on="cik", how="left")               # Join in ticker and company name
               [["cik","ticker","company_name","tag","period_type","ttm_cal_year","ttm_cal_quarter","ttm_value"]]  # Select/order cols
               .sort_values("ttm_value", ascending=False))        # Order by value descending for convenience
    vw.to_parquet(MARTS / "vw_latest_ttm.parquet")                # Persist the "view" parquet
    print("\nTop 10 latest TTM (any tag):")                       # Friendly console output for quick sanity check
    print(vw.head(10))                                            # Display top 10 rows
if __name__ == "__main__":                                       # Standard module guard (run main only if executed as script)
    main()                                                        # Execute the pipeline

# build_all_marts_v2.py ‚Äî compact, resumable, chunked
# pip install pandas pyarrow duckdb orjson

import os, sys, gc, time
from pathlib import Path
from datetime import datetime
from typing import Dict, Tuple
import pandas as pd
import pyarrow as pa, pyarrow.parquet as pq
import duckdb

# ------------------ fast json (optional) ------------------
try: import orjson as _json
except Exception: import json as _json

# ------------------ paths ------------------
BASE = Path(r"C:\smilefund_project")
FACTS = BASE / r"data\sec\company_facts"
DATA  = BASE / r"data\sec"
WARE  = BASE / r"warehouse\parquet"
MARTS = WARE / "marts_v2"
LOGS  = MARTS / "_logs"
TMP_DUCKDB = MARTS / "_duckdb_tmp"
for d in (MARTS, LOGS, TMP_DUCKDB): d.mkdir(parents=True, exist_ok=True)

# ------------------ tiny utils ------------------
def status(msg: str): print(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")
def sql_path(p: Path) -> str: return "'" + str(p).replace("'", "''") + "'"
def safe_unlink(p: Path):
    try: p.unlink(missing_ok=True)
    except PermissionError:
        for i in range(6):
            time.sleep(0.3*(i+1)); gc.collect()
            try: p.unlink(); break
            except PermissionError: pass

def set_duckdb(con, threads: int = None, memory: str = "8GB"):
    t = threads or max(1, (os.cpu_count() or 4)//2)
    con.execute(f"PRAGMA threads={t};")
    try: con.execute("PRAGMA temp_directory=?", [str(TMP_DUCKDB)])
    except Exception: pass
    con.execute(f"PRAGMA memory_limit='{memory}';")
    con.execute("SET preserve_insertion_order=false;")
    status(f"DuckDB threads={t}, mem={memory}, tmp={TMP_DUCKDB}")

# ------------------ constants / helpers ------------------
USD_UNITS_MULT: Dict[str,int] = {
    "USD":1,"USD$":1,"USDthousands":1_000,"USDThousands":1_000,
    "USDm":1_000_000,"USDmillions":1_000_000,"USDMillions":1_000_000
}
Q_MIN, Q_MAX = 70, 110
FY_MIN, FY_MAX = 330, 380
def parse_dt(s): return pd.to_datetime(s, format="%Y-%m-%d", errors="coerce")
def duration_days(s, e):
    s, e = parse_dt(s), parse_dt(e)
    return None if (pd.isna(s) or pd.isna(e)) else int((e - s).days) + 1

def load_company_file(path: Path) -> Tuple[int, dict]:
    try:     d = _json.loads(path.read_bytes())
    except:  d = _json.loads(path.read_text(encoding="utf-8"))
    return int(d.get("cik", 0)), d

def iter_all_us_gaap_usd(cik: int, data: dict):
    facts = data.get("facts", {}).get("us-gaap", {})
    for tag, blk in facts.items():
        for unit, arr in blk.get("units", {}).items():
            m = USD_UNITS_MULT.get(unit)
            if not m: continue
            for rec in arr:
                fp, form, fy, val = rec.get("fp"), rec.get("form"), rec.get("fy"), rec.get("val")
                if val is None or fy is None or fp is None or form is None: continue
                pt = "duration" if rec.get("start") else "instant"
                if pt == "duration":
                    d = duration_days(rec.get("start"), rec.get("end"))
                    if fp in ("Q1","Q2","Q3"):
                        if not (form and form.startswith("10-Q") and d and Q_MIN<=d<=Q_MAX): continue
                    elif fp=="FY":
                        if not (form and form.startswith("10-K") and d and FY_MIN<=d<=FY_MAX): continue
                    elif fp=="Q4": continue
                else:
                    if fp not in ("Q1","Q2","Q3","Q4","FY"): continue
                try: value = float(val)*m
                except: continue
                yield {"cik":cik,"tag":tag,"unit":unit,"fy":int(fy),"fp":str(fp),"form":str(form),
                       "filed":rec.get("filed"),"start":rec.get("start"),"end":rec.get("end"),
                       "period_type":pt,"value":value}

# ------------------ Phase A: JSON ‚Üí raw parquet (streaming) ------------------
def ingest_json_to_raw(raw_path: Path, buffer_rows: int = 15_000):
    arrow_schema = pa.schema([
        ("cik",pa.int64()),("tag",pa.string()),("unit",pa.string()),
        ("fy",pa.int32()),("fp",pa.string()),("form",pa.string()),
        ("filed",pa.string()),("start",pa.string()),("end",pa.string()),
        ("period_type",pa.string()),("value",pa.float64())
    ])
    buf, writer, nfiles = [], None, 0
    status("üì• Ingesting JSON ‚Üí raw parquet ‚Ä¶")
    for de in os.scandir(FACTS):
        if not de.is_file() or not de.name.endswith(".json"): continue
        nfiles += 1
        try:
            cik, data = load_company_file(Path(de.path))
            for row in iter_all_us_gaap_usd(cik, data): buf.append(row)
            if len(buf) >= buffer_rows:
                tbl = pa.Table.from_pylist(buf, schema=arrow_schema)
                if writer is None: writer = pq.ParquetWriter(raw_path, arrow_schema, compression="zstd")
                writer.write_table(tbl); buf.clear(); del tbl; gc.collect()
                status(f"flushed ~{buffer_rows:,} rows ‚Üí {raw_path.name}")
        except Exception: pass
        if nfiles % 2000 == 0: status(f"parsed {nfiles} companies")
    if buf:
        tbl = pa.Table.from_pylist(buf, schema=arrow_schema)
        if writer is None: writer = pq.ParquetWriter(raw_path, arrow_schema, compression="zstd")
        writer.write_table(tbl)
    if writer: writer.close()
    if not raw_path.exists(): raise RuntimeError("Raw parquet not created.")
    status(f"‚úÖ wrote {raw_path}")

# ------------------ Phase B/C: DuckDB transforms (chunked + resumable) ------------------
def duckdb_transforms(raw_path: Path,
                      best_path: Path,
                      qtrue_path: Path,
                      ttm_aligned_path: Path,
                      fye_path: Path,
                      latest_ttm_path: Path,
                      threads: int = None,
                      memory: str = "8GB"):
    with duckdb.connect() as con:
        set_duckdb(con, threads, memory)

        # temp view over raw parquet
        con.execute(f"CREATE OR REPLACE TEMP VIEW raw AS SELECT * FROM read_parquet({sql_path(raw_path)});")

        # ---- BEST: skip if parquet exists; else build per-tag and write parquet
        if best_path.exists():
            status(f"‚Ü™Ô∏è Found existing best, skipping rebuild: {best_path}")
            con.execute("DROP VIEW IF EXISTS best;")
            con.execute(f"CREATE VIEW best AS SELECT * FROM read_parquet({sql_path(best_path)});")
        else:
            status("Building 'best' per tag (chunked) ‚Ä¶")
            con.execute("DROP TABLE IF EXISTS best;")
            con.execute("""
                CREATE TABLE best (
                    cik BIGINT, tag VARCHAR, unit VARCHAR, fy INTEGER, fp VARCHAR,
                    form VARCHAR, filed VARCHAR, start VARCHAR, "end" VARCHAR,
                    period_type VARCHAR, value DOUBLE
                );
            """)
            tags = [r[0] for r in con.execute("SELECT DISTINCT tag FROM raw").fetchall()]
            status(f"{len(tags):,} tags to process")
            for i, t in enumerate(tags, 1):
                con.execute(f"""
                    WITH ranked AS (
                      SELECT *,
                             ROW_NUMBER() OVER (
                               PARTITION BY cik, tag, fy, fp, period_type
                               ORDER BY filed ASC
                             ) AS rn
                      FROM raw WHERE tag = {sql_path(t)}
                    )
                    INSERT INTO best
                    SELECT cik, tag, unit, fy, fp, form, filed, start, "end", period_type, value
                    FROM ranked WHERE rn=1;
                """)
                if i % 50 == 0:
                    status(f"‚Ä¶ {i}/{len(tags)} tags"); con.execute("CHECKPOINT;")
            con.execute(f"COPY best TO {sql_path(best_path)} (FORMAT PARQUET, COMPRESSION ZSTD);")
            status(f"‚úÖ wrote {best_path}")
            con.execute("DROP VIEW IF EXISTS best;")
            con.execute("DROP TABLE IF EXISTS best;")
            con.execute(f"CREATE VIEW best AS SELECT * FROM read_parquet({sql_path(best_path)});")

        # ---- quarters_true
        safe_unlink(qtrue_path)
        con.execute("""CREATE OR REPLACE TABLE quarters_true AS
                       WITH dur_q AS (
                         SELECT cik, tag, fy, fp,
                                CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 END AS qn,
                                value, period_type
                         FROM best
                         WHERE period_type='duration' AND fp IN ('Q1','Q2','Q3')
                       ),
                       dur_fy AS (
                         SELECT cik, tag, fy, value AS fy_value
                         FROM best
                         WHERE period_type='duration' AND fp='FY'
                       ),
                       dur_qsum AS (
                         SELECT cik, tag, fy, SUM(value) AS q123
                         FROM dur_q
                         GROUP BY 1,2,3
                       ),
                       dur_q4_delta AS (
                         SELECT f.cik, f.tag, f.fy, '10K delta' AS fp, 4 AS qn,
                                (f.fy_value - q.q123) AS value, 'duration' AS period_type
                         FROM dur_fy f JOIN dur_qsum q USING(cik, tag, fy)
                       ),
                       ins_q AS (
                         SELECT cik, tag, fy, fp,
                                CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 END AS qn,
                                value, period_type
                         FROM best
                         WHERE period_type='instant' AND fp IN ('Q1','Q2','Q3','Q4')
                       ),
                       ins_fy AS (SELECT cik, tag, fy, value FROM best WHERE period_type='instant' AND fp='FY'),
                       have_q4 AS (SELECT DISTINCT cik, tag, fy FROM ins_q WHERE fp='Q4'),
                       ins_fy_to_q4 AS (
                         SELECT f.cik, f.tag, f.fy, 'Q4' AS fp, 4 AS qn, f.value, 'instant' AS period_type
                         FROM ins_fy f LEFT JOIN have_q4 h USING(cik, tag, fy) WHERE h.cik IS NULL
                       )
                       SELECT * FROM dur_q
                       UNION ALL SELECT * FROM dur_q4_delta
                       UNION ALL SELECT * FROM ins_q
                       UNION ALL SELECT * FROM ins_fy_to_q4;""")
        con.execute(f"COPY quarters_true TO {sql_path(qtrue_path)} (FORMAT PARQUET, COMPRESSION ZSTD);")
        status(f"‚úÖ wrote {qtrue_path}")

        # ---- ttm (rolling for duration; identity for instant)
        con.execute("""CREATE OR REPLACE TABLE ttm AS
                       WITH base AS (SELECT cik, tag, fy, fp, qn, value, period_type FROM quarters_true),
                       w AS (
                         SELECT *,
                           CASE WHEN period_type='duration'
                                THEN SUM(value) OVER (PARTITION BY cik, tag, period_type ORDER BY fy, qn ROWS BETWEEN 3 PRECEDING AND CURRENT ROW)
                                ELSE value END AS TTM_value,
                           CASE WHEN period_type='duration'
                                THEN COUNT(value) OVER (PARTITION BY cik, tag, period_type ORDER BY fy, qn ROWS BETWEEN 3 PRECEDING AND CURRENT ROW)
                                ELSE 1 END AS cnt4
                         FROM base
                       )
                       SELECT cik, tag, fy, fp, qn, value, period_type, TTM_value
                       FROM w
                       WHERE (period_type='instant' OR cnt4=4) AND TTM_value IS NOT NULL;""")

        # ---- fiscal year end month  (FIX: quote "end")
        safe_unlink(fye_path)
        con.execute("""CREATE OR REPLACE TABLE dim_fiscal_year_end AS
                       WITH fy AS (
                         SELECT cik, TRY_CAST("end" AS DATE) AS end_dt
                         FROM best
                         WHERE period_type='duration' AND fp='FY' AND "end" IS NOT NULL
                       ),
                       months AS (
                         SELECT cik, EXTRACT(MONTH FROM end_dt)::INT AS fye_month, COUNT(*) AS cnt
                         FROM fy WHERE end_dt IS NOT NULL
                         GROUP BY 1,2
                       ),
                       ranked AS (
                         SELECT *, ROW_NUMBER() OVER (PARTITION BY cik ORDER BY cnt DESC, fye_month DESC) AS rn
                         FROM months
                       )
                       SELECT cik, fye_month, ((fye_month % 12)+1)::INT AS fiscal_year_start_month
                       FROM ranked WHERE rn=1;""")
        con.execute(f"COPY dim_fiscal_year_end TO {sql_path(fye_path)} (FORMAT PARQUET, COMPRESSION ZSTD);")
        status(f"‚úÖ wrote {fye_path}")

        # ---- align TTM to calendar quarter/year
        safe_unlink(ttm_aligned_path)
        con.execute("""CREATE OR REPLACE TABLE ttm_aligned AS
                       WITH a AS (SELECT t.*, d.fye_month FROM ttm t LEFT JOIN dim_fiscal_year_end d USING(cik)),
                       aln AS (
                         SELECT a.*,
                                (((fye_month + CASE qn WHEN 4 THEN 0 WHEN 3 THEN -3 WHEN 2 THEN -6 WHEN 1 THEN -9 END) - 1) % 12 + 1) AS quarter_end_month
                         FROM a WHERE fye_month IS NOT NULL AND qn IS NOT NULL
                       )
                       SELECT *,
                              ((quarter_end_month - 1)/3 + 1)::INT AS calendar_quarter,
                              CASE WHEN quarter_end_month <= fye_month THEN fy ELSE fy - 1 END AS calendar_year
                       FROM aln;""")
        con.execute(f"COPY ttm_aligned TO {sql_path(ttm_aligned_path)} (FORMAT PARQUET, COMPRESSION ZSTD);")
        status(f"‚úÖ wrote {ttm_aligned_path}")

        # ---- final latest TTM (needs dim_security if available)
        dim_sec_path = MARTS / "dim_security.parquet"
        if dim_sec_path.exists():
            con.execute(f"CREATE OR REPLACE VIEW dim_security AS SELECT * FROM read_parquet({sql_path(dim_sec_path)});")
        else:
            con.execute("""CREATE OR REPLACE VIEW dim_security AS
                           SELECT CAST(NULL AS BIGINT) cik, CAST(NULL AS VARCHAR) ticker, CAST(NULL AS VARCHAR) company_name
                           WHERE 1=0;""")
        con.execute(f"CREATE OR REPLACE VIEW ttm_aligned AS SELECT * FROM read_parquet({sql_path(ttm_aligned_path)});")

        safe_unlink(latest_ttm_path)
        con.execute("""CREATE OR REPLACE TABLE vw_latest_ttm AS
                       WITH a AS (
                         SELECT cik, tag, period_type, calendar_year, calendar_quarter, TTM_value
                         FROM ttm_aligned
                         WHERE calendar_year IS NOT NULL AND calendar_quarter IS NOT NULL
                       ),
                       ranked AS (
                         SELECT a.*, ROW_NUMBER() OVER (
                           PARTITION BY cik, tag ORDER BY calendar_year DESC, calendar_quarter DESC
                         ) AS rn
                         FROM a
                       )
                       SELECT r.cik, s.ticker, s.company_name, r.tag, r.period_type,
                              r.calendar_year AS ttm_cal_year, r.calendar_quarter AS ttm_cal_quarter,
                              r.TTM_value AS ttm_value
                       FROM ranked r LEFT JOIN dim_security s USING(cik)
                       WHERE rn=1
                       ORDER BY ttm_value DESC NULLS LAST;""")
        con.execute(f"COPY vw_latest_ttm TO {sql_path(latest_ttm_path)} (FORMAT PARQUET, COMPRESSION ZSTD);")
        status(f"‚úÖ wrote {latest_ttm_path}")

# ------------------ dim_security (tiny; pandas) ------------------
def build_dim_security(out_path: Path):
    rows, i = [], 0
    for de in os.scandir(FACTS):
        if not de.is_file() or not de.name.endswith(".json"): continue
        i += 1
        try:
            d = _json.loads(Path(de.path).read_bytes())
            cik = int(d.get("cik", 0))
            name = d.get("entityName") or ""
            ticker = d.get("ticker") or d.get("tickers")
            if isinstance(ticker, list) and ticker: ticker = str(ticker[0])
            rows.append({"cik":cik,"company_name":name,"ticker":ticker})
        except Exception: pass
        if i % 2000 == 0: status(f"dim_security progress {i}+")
    dim = pd.DataFrame(rows).drop_duplicates("cik")
    dim["cik"] = pd.to_numeric(dim["cik"], errors="coerce").astype("Int64")
    dim["ticker"] = dim["ticker"].astype(str).str.upper().str.strip()
    dim.loc[dim["ticker"].isin(["NONE","NAN","<NA>",""]), "ticker"] = pd.NA
    dim["company_name"] = (dim["company_name"].astype(str)
                           .str.replace(r"[^A-Za-z0-9 &\.-]+"," ",regex=True)
                           .str.replace(r"\s+"," ",regex=True).str.strip())
    map_csv = DATA / "ticker_map.csv"
    if map_csv.exists():
        tm = pd.read_csv(map_csv)
        tm.columns = [c.strip().lower() for c in tm.columns]
        if {"cik","ticker"} <= set(tm.columns):
            tm["cik"] = pd.to_numeric(tm["cik"], errors="coerce").astype("Int64")
            tm["ticker"] = tm["ticker"].astype(str).str.upper().str.strip().replace("-", ".", regex=False)
            tm = tm.dropna(subset=["cik"]).drop_duplicates("cik", keep="first")
            dim = dim.merge(tm[["cik","ticker"]], on="cik", how="left", suffixes=("","_map"))
            dim["ticker"] = dim["ticker_map"].combine_first(dim["ticker"]); dim.drop(columns=["ticker_map"], inplace=True, errors="ignore")
            status(f"‚úÖ merged ticker_map.csv with {len(tm):,} rows")
        else:
            status("‚ö†Ô∏è ticker_map.csv missing 'cik' or 'ticker' ‚Äî skipped merge")
    else:
        status("‚ÑπÔ∏è ticker_map.csv not found ‚Äî dim_security uses embedded tickers only")
    dim.to_parquet(out_path)
    status(f"‚úÖ wrote {out_path}  rows={len(dim):,}")

# ------------------ orchestrator ------------------
def main():
    status("üöÄ build_all_marts_v2 started")
    raw_p   = MARTS / "_raw_us_gaap_usd.parquet"
    best_p  = MARTS / "_best.parquet"
    qtrue_p = MARTS / "fact_quarters_true.parquet"
    ttm_aln = MARTS / "fact_ttm_aligned.parquet"
    fye_p   = MARTS / "dim_fiscal_year_end.parquet"
    latest  = MARTS / "vw_latest_ttm.parquet"
    dimsec  = MARTS / "dim_security.parquet"

    # Phase A (skip if raw exists)
    if raw_p.exists():
        status(f"‚Ü™Ô∏è Found existing raw, skipping ingest: {raw_p}")
    else:
        safe_unlink(best_p)  # ensure clean downstream when rebuilding raw
        ingest_json_to_raw(raw_p, buffer_rows=15_000)

    # Phase B/C (chunked + resumable on best)
    duckdb_transforms(raw_p, best_p, qtrue_p, ttm_aln, fye_p, latest, threads=2, memory="8GB")

    # dim_security
    if not dimsec.exists():
        build_dim_security(dimsec)
    else:
        status(f"‚Ü™Ô∏è Found existing dim_security, skipping: {dimsec}")

    # quick preview
    try:
        vw = pd.read_parquet(latest)
        print("\nTop 10 latest TTM (any tag):")
        print(vw.head(10))
    except Exception as e:
        status(f"Note: preview failed: {e}")

    status("üèÅ build_all_marts_v2 finished")

# ------------------ run ------------------
if __name__ == "__main__":
    # Make it Jupyter-friendly (ignore kernel args)
    if any(k in sys.modules for k in ("ipykernel","IPython")):
        sys.argv = ["build_all_marts_v2.py"]
    main()

pip install pandas pyarrow duckdb orjson  # (terminal) install deps, not a Python statement
import os, sys, gc, time  # stdlib: OS ops, argv/modules, GC, timing
from pathlib import Path  # path handling
from datetime import datetime  # timestamps for logging
from typing import Dict, Tuple  # typing hints
import pandas as pd  # dataframes
import pyarrow as pa, pyarrow.parquet as pq  # Arrow types + Parquet IO
import duckdb  # embedded analytics DB
try: import orjson as _json  # fast JSON
except Exception: import json as _json  # fallback JSON
BASE = Path(r"C:\smilefund_project")  # project root
FACTS = BASE / r"data\sec\company_facts"  # SEC company_facts JSON dir
DATA  = BASE / r"data\sec"  # SEC data dir
WARE  = BASE / r"warehouse\parquet"  # warehouse root
MARTS = WARE / "marts_v2"  # marts versioned folder
LOGS  = MARTS / "_logs"  # logs folder
TMP_DUCKDB = MARTS / "_duckdb_tmp"  # temp dir for DuckDB
for d in (MARTS, LOGS, TMP_DUCKDB): d.mkdir(parents=True, exist_ok=True)  # ensure dirs
def status(msg: str): print(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")  # quick logger
def sql_path(p: Path) -> str: return "'" + str(p).replace("'", "''") + "'"  # escape path for SQL
def safe_unlink(p: Path):  # robust delete (handles Windows locks)
    try: p.unlink(missing_ok=True)  # try once
    except PermissionError:  # if locked, retry with backoff
        for i in range(6):
            time.sleep(0.3*(i+1)); gc.collect()  # wait and GC
            try: p.unlink(); break  # success → stop
            except PermissionError: pass  # keep trying
def set_duckdb(con, threads: int = None, memory: str = "8GB"):  # tune DuckDB session
    t = threads or max(1, (os.cpu_count() or 4)//2)  # default threads ~ half cores
    con.execute(f"PRAGMA threads={t};")  # set threads
    try: con.execute("PRAGMA temp_directory=?", [str(TMP_DUCKDB)])  # temp dir
    except Exception: pass  # older versions may not support
    con.execute(f"PRAGMA memory_limit='{memory}';")  # mem cap
    con.execute("SET preserve_insertion_order=false;")  # perf tweak
    status(f"DuckDB threads={t}, mem={memory}, tmp={TMP_DUCKDB}")  # log cfg
USD_UNITS_MULT: Dict[str,int] = {  # unit scalars for USD values
    "USD":1,"USD$":1,"USDthousands":1_000,"USDThousands":1_000,
    "USDm":1_000_000,"USDmillions":1_000_000,"USDMillions":1_000_000
}
Q_MIN, Q_MAX = 70, 110  # valid quarter day span
FY_MIN, FY_MAX = 330, 380  # valid fiscal-year day span
def parse_dt(s): return pd.to_datetime(s, format="%Y-%m-%d", errors="coerce")  # safe parse y-m-d
def duration_days(s, e):  # inclusive day count or None
    s, e = parse_dt(s), parse_dt(e)
    return None if (pd.isna(s) or pd.isna(e)) else int((e - s).days) + 1
def load_company_file(path: Path) -> Tuple[int, dict]:  # load one company_facts JSON
    try:     d = _json.loads(path.read_bytes())  # binary read (fast)
    except:  d = _json.loads(path.read_text(encoding="utf-8"))  # text fallback
    return int(d.get("cik", 0)), d  # CIK + full dict
def iter_all_us_gaap_usd(cik: int, data: dict):  # yield normalized US-GAAP USD facts
    facts = data.get("facts", {}).get("us-gaap", {})  # section
    for tag, blk in facts.items():  # each metric tag
        for unit, arr in blk.get("units", {}).items():  # per unit
            m = USD_UNITS_MULT.get(unit)  # multiplier
            if not m: continue  # skip non-USD
            for rec in arr:  # each reported record
                fp, form, fy, val = rec.get("fp"), rec.get("form"), rec.get("fy"), rec.get("val")  # basics
                if val is None or fy is None or fp is None or form is None: continue  # need all
                pt = "duration" if rec.get("start") else "instant"  # has start→duration
                if pt == "duration":  # validate period lengths/forms
                    d = duration_days(rec.get("start"), rec.get("end"))  # span
                    if fp in ("Q1","Q2","Q3"):
                        if not (form and form.startswith("10-Q") and d and Q_MIN<=d<=Q_MAX): continue  # good 10-Q
                    elif fp=="FY":
                        if not (form and form.startswith("10-K") and d and FY_MIN<=d<=FY_MAX): continue  # good 10-K
                    elif fp=="Q4": continue  # ignore duration Q4 (derive delta)
                else:
                    if fp not in ("Q1","Q2","Q3","Q4","FY"): continue  # valid instant fps
                try: value = float(val)*m  # scale to base USD
                except: continue  # non-numeric → skip
                yield {"cik":cik,"tag":tag,"unit":unit,"fy":int(fy),"fp":str(fp),"form":str(form),  # record out
                       "filed":rec.get("filed"),"start":rec.get("start"),"end":rec.get("end"),
                       "period_type":pt,"value":value}
def ingest_json_to_raw(raw_path: Path, buffer_rows: int = 30_000):  # stream JSON→Parquet
    arrow_schema = pa.schema([  # fixed schema for raw
        ("cik",pa.int64()),("tag",pa.string()),("unit",pa.string()),
        ("fy",pa.int32()),("fp",pa.string()),("form",pa.string()),
        ("filed",pa.string()),("start",pa.string()),("end",pa.string()),
        ("period_type",pa.string()),("value",pa.float64())
    ])
    buf, writer, nfiles = [], None, 0  # init buffer/state
    for de in os.scandir(FACTS):  # iterate company files
        if not de.is_file() or not de.name.endswith(".json"): continue  # only JSON files
        nfiles += 1  # count files
        try:
            cik, data = load_company_file(Path(de.path))  # load JSON
            for row in iter_all_us_gaap_usd(cik, data): buf.append(row)  # collect rows
            if len(buf) >= buffer_rows:  # flush chunk
                tbl = pa.Table.from_pylist(buf, schema=arrow_schema)  # to Arrow
                if writer is None: writer = pq.ParquetWriter(raw_path, arrow_schema, compression="zstd")  # open writer
                writer.write_table(tbl); buf.clear(); del tbl; gc.collect()  # write+clear
                status(f"flushed ~{buffer_rows:,} rows → {raw_path.name}")  # log flush
        except Exception: pass  # skip bad file but continue
        if nfiles % 2000 == 0: status(f"parsed {nfiles} companies")  # progress log
    if buf:  # final flush
        tbl = pa.Table.from_pylist(buf, schema=arrow_schema)
        if writer is None: writer = pq.ParquetWriter(raw_path, arrow_schema, compression="zstd")
        writer.write_table(tbl)
    if writer: writer.close()  # close parquet writer
    if not raw_path.exists(): raise RuntimeError("Raw parquet not created.")  # guard
def duckdb_transforms(raw_path: Path,  # transform pipeline
                      best_path: Path,
                      qtrue_path: Path,
                      ttm_aligned_path: Path,
                      fye_path: Path,
                      latest_ttm_path: Path,
                      threads: int = None,
                      memory: str = "8GB"):
    with duckdb.connect() as con:  # open connection
        set_duckdb(con, threads, memory)  # configure engine
        con.execute(f"CREATE OR REPLACE TEMP VIEW raw AS SELECT * FROM read_parquet({sql_path(raw_path)});")  # temp view
        if best_path.exists():  # if cached "best" exists, rebuild; else compute fresh below
            con.execute("DROP VIEW IF EXISTS best;")  # cleanup
            con.execute(f"CREATE VIEW best AS SELECT * FROM read_parquet({sql_path(best_path)});")  # expose cache
            con.execute("DROP TABLE IF EXISTS best;")  # ensure table name free
            con.execute("""  # table to hold earliest-filed per (cik,tag,fy,fp,period_type)
                CREATE TABLE best (
                    cik BIGINT, tag VARCHAR, unit VARCHAR, fy INTEGER, fp VARCHAR,
                    form VARCHAR, filed VARCHAR, start VARCHAR, "end" VARCHAR,
                    period_type VARCHAR, value DOUBLE
                );
            """)
            tags = [r[0] for r in con.execute("SELECT DISTINCT tag FROM raw").fetchall()]  # all tags
            status(f"{len(tags):,} tags to process")  # progress
            for i, t in enumerate(tags, 1):  # per tag, dedupe by earliest filed
                con.execute(f"""
                    WITH ranked AS (
                      SELECT *,
                             ROW_NUMBER() OVER (
                               PARTITION BY cik, tag, fy, fp, period_type
                               ORDER BY filed ASC
                             ) AS rn
                      FROM raw WHERE tag = {sql_path(t)}
                    )
                    INSERT INTO best
                    SELECT cik, tag, unit, fy, fp, form, filed, start, "end", period_type, value
                    FROM ranked WHERE rn=1;
                """)
                if i % 50 == 0:
                    status(f"… {i}/{len(tags)} tags"); con.execute("CHECKPOINT;")  # periodic checkpoint
            con.execute(f"COPY best TO {sql_path(best_path)} (FORMAT PARQUET, COMPRESSION ZSTD);")  # persist best
            con.execute("DROP VIEW IF EXISTS best;")  # cleanup
            con.execute("DROP TABLE IF EXISTS best;")
            con.execute(f"CREATE VIEW best AS SELECT * FROM read_parquet({sql_path(best_path)});")  # reload view
        safe_unlink(qtrue_path)  # remove old quarters_true
        con.execute("""CREATE OR REPLACE TABLE quarters_true AS  -- construct quarterized facts
                       WITH dur_q AS (  -- Q1-Q3 duration values
                         SELECT cik, tag, fy, fp,
                                CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 END AS qn,
                                value, period_type
                         FROM best
                         WHERE period_type='duration' AND fp IN ('Q1','Q2','Q3')
                       ),
                       dur_fy AS (  -- full-year duration
                         SELECT cik, tag, fy, value AS fy_value
                         FROM best
                         WHERE period_type='duration' AND fp='FY'
                       ),
                       dur_qsum AS (  -- sum Q1-Q3 per FY
                         SELECT cik, tag, fy, SUM(value) AS q123
                         FROM dur_q
                         GROUP BY 1,2,3
                       ),
                       dur_q4_delta AS (  -- derive Q4 = FY - (Q1+Q2+Q3)
                         SELECT f.cik, f.tag, f.fy, '10K delta' AS fp, 4 AS qn,
                                (f.fy_value - q.q123) AS value, 'duration' AS period_type
                         FROM dur_fy f JOIN dur_qsum q USING(cik, tag, fy)
                       ),
                       ins_q AS (  -- instant values Q1-Q4
                         SELECT cik, tag, fy, fp,
                                CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 END AS qn,
                                value, period_type
                         FROM best
                         WHERE period_type='instant' AND fp IN ('Q1','Q2','Q3','Q4')
                       ),
                       ins_fy AS (SELECT cik, tag, fy, value FROM best WHERE period_type='instant' AND fp='FY'),  -- FY instant
                       have_q4 AS (SELECT DISTINCT cik, tag, fy FROM ins_q WHERE fp='Q4'),  # check Q4 presence
                       ins_fy_to_q4 AS (  -- if no Q4 instant, copy FY as Q4
                         SELECT f.cik, f.tag, f.fy, 'Q4' AS fp, 4 AS qn, f.value, 'instant' AS period_type
                         FROM ins_fy f LEFT JOIN have_q4 h USING(cik, tag, fy) WHERE h.cik IS NULL
                       )
                       SELECT * FROM dur_q
                       UNION ALL SELECT * FROM dur_q4_delta
                       UNION ALL SELECT * FROM ins_q
                       UNION ALL SELECT * FROM ins_fy_to_q4;""")
        con.execute(f"COPY quarters_true TO {sql_path(qtrue_path)} (FORMAT PARQUET, COMPRESSION ZSTD);")  # save
        con.execute("""CREATE OR REPLACE TABLE ttm AS  -- rolling TTM for duration, pass-through for instant
                       WITH base AS (SELECT cik, tag, fy, fp, qn, value, period_type FROM quarters_true),
                       w AS (
                         SELECT *,
                           CASE WHEN period_type='duration'
                                THEN SUM(value) OVER (PARTITION BY cik, tag, period_type ORDER BY fy, qn ROWS BETWEEN 3 PRECEDING AND CURRENT ROW)
                                ELSE value END AS TTM_value,
                           CASE WHEN period_type='duration'
                                THEN COUNT(value) OVER (PARTITION BY cik, tag, period_type ORDER BY fy, qn ROWS BETWEEN 3 PRECEDING AND CURRENT ROW)
                                ELSE 1 END AS cnt4
                         FROM base
                       )
                       SELECT cik, tag, fy, fp, qn, value, period_type, TTM_value
                       FROM w
                       WHERE (period_type='instant' OR cnt4=4) AND TTM_value IS NOT NULL;""")
        safe_unlink(fye_path)  # remove old dim
        con.execute("""CREATE OR REPLACE TABLE dim_fiscal_year_end AS  -- detect dominant FY-end month per CIK
                       WITH fy AS (
                         SELECT cik, TRY_CAST("end" AS DATE) AS end_dt
                         FROM best
                         WHERE period_type='duration' AND fp='FY' AND "end" IS NOT NULL
                       ),
                       months AS (
                         SELECT cik, EXTRACT(MONTH FROM end_dt)::INT AS fye_month, COUNT(*) AS cnt
                         FROM fy WHERE end_dt IS NOT NULL
                         GROUP BY 1,2
                       ),
                       ranked AS (
                         SELECT *, ROW_NUMBER() OVER (PARTITION BY cik ORDER BY cnt DESC, fye_month DESC) AS rn
                         FROM months
                       )
                       SELECT cik, fye_month, ((fye_month % 12)+1)::INT AS fiscal_year_start_month
                       FROM ranked WHERE rn=1;""")
        con.execute(f"COPY dim_fiscal_year_end TO {sql_path(fye_path)} (FORMAT PARQUET, COMPRESSION ZSTD);")  # save

        safe_unlink(ttm_aligned_path)  # remove old aligned
        con.execute("""CREATE OR REPLACE TABLE ttm_aligned AS  -- align TTM to calendar year/quarter
                       WITH a AS (SELECT t.*, d.fye_month FROM ttm t LEFT JOIN dim_fiscal_year_end d USING(cik)),
                       aln AS (
                         SELECT a.*,
                                (((fye_month + CASE qn WHEN 4 THEN 0 WHEN 3 THEN -3 WHEN 2 THEN -6 WHEN 1 THEN -9 END) - 1) % 12 + 1) AS quarter_end_month
                         FROM a WHERE fye_month IS NOT NULL AND qn IS NOT NULL
                       )
                       SELECT *,
                              ((quarter_end_month - 1)/3 + 1)::INT AS calendar_quarter,
                              CASE WHEN quarter_end_month <= fye_month THEN fy ELSE fy - 1 END AS calendar_year
                       FROM aln;""")
        con.execute(f"COPY ttm_aligned TO {sql_path(ttm_aligned_path)} (FORMAT PARQUET, COMPRESSION ZSTD);")  # save
        con.execute("DROP TABLE IF EXISTS ttm_aligned;")  # keep as view
        con.execute(f"CREATE OR REPLACE VIEW ttm_aligned AS SELECT * FROM read_parquet({sql_path(ttm_aligned_path)});")  # expose view
        dim_sec_path = MARTS / "dim_security.parquet"  # expected dim path
        if dim_sec_path.exists():
            con.execute(f"CREATE OR REPLACE VIEW dim_security AS SELECT * FROM read_parquet({sql_path(dim_sec_path)});")  # load if exists
        else:
            con.execute("""CREATE OR REPLACE VIEW dim_security AS
                           SELECT CAST(NULL AS BIGINT) cik, CAST(NULL AS VARCHAR) ticker, CAST(NULL AS VARCHAR) company_name
                           WHERE 1=0;""")  # empty schema view for joins
        safe_unlink(latest_ttm_path)  # remove old latest
        con.execute("""CREATE OR REPLACE TABLE vw_latest_ttm AS  -- latest TTM per (cik, tag)
                       WITH a AS (
                         SELECT cik, tag, period_type, calendar_year, calendar_quarter, TTM_value
                         FROM ttm_aligned
                         WHERE calendar_year IS NOT NULL AND calendar_quarter IS NOT NULL
                       ),
                       ranked AS (
                         SELECT a.*, ROW_NUMBER() OVER (
                           PARTITION BY cik, tag ORDER BY calendar_year DESC, calendar_quarter DESC
                         ) AS rn
                         FROM a
                       )
                       SELECT r.cik, s.ticker, s.company_name, r.tag, r.period_type,
                              r.calendar_year AS ttm_cal_year, r.calendar_quarter AS ttm_cal_quarter,
                              r.TTM_value AS ttm_value
                       FROM ranked r LEFT JOIN dim_security s USING(cik)
                       WHERE rn=1
                       ORDER BY ttm_value DESC NULLS LAST;""")
        con.execute(f"COPY vw_latest_ttm TO {sql_path(latest_ttm_path)} (FORMAT PARQUET, COMPRESSION ZSTD);")  # save
def build_dim_security(out_path: Path):  # build dim_security from JSON + optional map
    rows, i = [], 0  # accumulators
    for de in os.scandir(FACTS):  # scan facts dir
        if not de.is_file() or not de.name.endswith(".json"): continue  # only JSON
        i += 1  # count
        try:
            d = _json.loads(Path(de.path).read_bytes())  # load json
            cik = int(d.get("cik", 0))  # CIK
            name = d.get("entityName") or ""  # company name
            ticker = d.get("ticker") or d.get("tickers")  # ticker or list
            if isinstance(ticker, list) and ticker: ticker = str(ticker[0])  # take first
            rows.append({"cik":cik,"company_name":name,"ticker":ticker})  # add row
        except Exception: pass  # skip bad file
        if i % 2000 == 0: status(f"dim_security progress {i}+")  # progress log
    dim = pd.DataFrame(rows).drop_duplicates("cik")  # unique by CIK
    dim["cik"] = pd.to_numeric(dim["cik"], errors="coerce").astype("Int64")  # clean CIK
    dim["ticker"] = dim["ticker"].astype(str).str.upper().str.strip()  # normalize ticker
    dim.loc[dim["ticker"].isin(["NONE","NAN","<NA>",""]), "ticker"] = pd.NA  # null out junk
    dim["company_name"] = (dim["company_name"].astype(str)  # clean name
                           .str.replace(r"[^A-Za-z0-9 &\.-]+"," ",regex=True)
                           .str.replace(r"\s+"," ",regex=True).str.strip())
    map_csv = DATA / "ticker_map.csv"  # optional override map
    if map_csv.exists():
        tm = pd.read_csv(map_csv)  # read map
        tm.columns = [c.strip().lower() for c in tm.columns]  # normalize cols
        if {"cik","ticker"} <= set(tm.columns):  # require keys
            tm["cik"] = pd.to_numeric(tm["cik"], errors="coerce").astype("Int64")  # clean CIK
            tm["ticker"] = tm["ticker"].astype(str).str.upper().str.strip().replace("-", ".", regex=False)  # normalize
            tm = tm.dropna(subset=["cik"]).drop_duplicates("cik", keep="first")  # unique
            dim = dim.merge(tm[["cik","ticker"]], on="cik", how="left", suffixes=("","_map"))  # left join
            dim["ticker"] = dim["ticker_map"].combine_first(dim["ticker"]); dim.drop(columns=["ticker_map"], inplace=True, errors="ignore")  # apply map
    dim.to_parquet(out_path)  # write parquet
def main():  # orchestration
    raw_p   = MARTS / "_raw_us_gaap_usd.parquet"  # raw facts parquet
    best_p  = MARTS / "_best.parquet"  # deduped earliest-filed
    qtrue_p = MARTS / "fact_quarters_true.parquet"  # quarterized facts
    ttm_aln = MARTS / "fact_ttm_aligned.parquet"  # calendar-aligned TTM
    fye_p   = MARTS / "dim_fiscal_year_end.parquet"  # fiscal year-end dim
    latest  = MARTS / "vw_latest_ttm.parquet"  # latest TTM view export
    dimsec  = MARTS / "dim_security.parquet"  # dim_security parquet
    if raw_p.exists():  # reuse raw if present
        status(f"found existing raw, skipping ingest: {raw_p}")  # log reuse
    else:
        safe_unlink(best_p)  # clear best cache if any
        ingest_json_to_raw(raw_p, buffer_rows=30_000)  # build raw parquet
    duckdb_transforms(raw_p, best_p, qtrue_p, ttm_aln, fye_p, latest, threads=2, memory="8GB")  # run transforms
    if not dimsec.exists():  # create dim_security once
        build_dim_security(dimsec)
    try:
        vw = pd.read_parquet(latest)  # quick touch/read to verify file exists
    except Exception as e:
        status(f"warn: unable to read latest TTM → {e}")  # warn if unreadable
if __name__ == "__main__":  # CLI / notebook entry
    if any(k in sys.modules for k in ("ipykernel","IPython")):  # normalize argv in notebooks
        sys.argv = ["build_all_marts_v2.py"]  # pretend script name
    main()  # run

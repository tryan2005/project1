# build_all_marts_v2.py  # Efficient build: streaming + DuckDB + orjson (fallback)
# Install once: pip install pandas pyarrow duckdb orjson  # orjson optional (falls back to json)

import os, gc  # OS helpers and GC for occasional cleanup
from pathlib import Path  # Path-safe filesystem ops
from typing import Dict, Tuple  # Type hints
import pandas as pd  # Light previews + dim_security write
import pyarrow as pa  # Arrow tables
import pyarrow.parquet as pq  # Parquet writer
import duckdb  # Fast SQL over Parquet

try: import orjson as _json  # Fast JSON (Rust); returns Python objects
except Exception: import json as _json  # Fallback to stdlib json

BASE = Path(r"C:\smilefund_project")  # Project root
FACTS = BASE / r"data\sec\company_facts"  # SEC company_facts JSON folder
DATA = BASE / r"data\sec"  # Misc SEC data (ticker_map.csv)
WARE = BASE / r"warehouse\parquet"  # Warehouse root
MARTS = WARE / "marts_v2"  # Output mart folder
LOGS = MARTS / "_logs"  # Logs folder (reserved)
TMP_DUCKDB = MARTS / "_duckdb_tmp"  # DuckDB spill dir

MARTS.mkdir(parents=True, exist_ok=True)  # Ensure output dirs
LOGS.mkdir(parents=True, exist_ok=True)
TMP_DUCKDB.mkdir(parents=True, exist_ok=True)

USD_UNITS_MULT: Dict[str,int] = {"USD":1,"USD$":1,"USDthousands":1_000,"USDThousands":1_000,"USDm":1_000_000,"USDmillions":1_000_000,"USDMillions":1_000_000}  # Unit multipliers
Q_MIN, Q_MAX = 70, 110  # Valid 10-Q duration days
FY_MIN, FY_MAX = 330, 380  # Valid 10-K duration days

def parse_dt(s): return pd.to_datetime(s, format="%Y-%m-%d", errors="coerce")  # Parse YYYY-MM-DD → Timestamp/NaT
def duration_days(start, end):  # Inclusive day count
    s, e = parse_dt(start), parse_dt(end)
    if pd.isna(s) or pd.isna(e): return None
    return int((e - s).days) + 1
def is_duration(rec: dict) -> bool: return rec.get("start") is not None  # Duration has 'start'
def load_company_file(path: Path) -> Tuple[int, dict]:  # Parse company_facts JSON
    try: d = _json.loads(path.read_bytes())  # orjson fast path (bytes)
    except Exception: d = _json.loads(path.read_text(encoding="utf-8"))  # Fallback to text
    return int(d.get("cik", 0)), d  # Return cik and dict

def iter_all_us_gaap_usd(cik: int, data: dict):  # Stream normalized USD us-gaap rows
    facts = data.get("facts", {}).get("us-gaap", {})  # Dive to us-gaap
    mul = USD_UNITS_MULT  # Local ref (micro-optimization)
    for tag, blk in facts.items():
        units = blk.get("units", {})
        for unit, arr in units.items():
            m = mul.get(unit)
            if not m: continue
            for rec in arr:
                fp, form, fy, val = rec.get("fp"), rec.get("form"), rec.get("fy"), rec.get("val")
                if val is None or fy is None or fp is None or form is None: continue
                pt = "duration" if rec.get("start") else "instant"
                if pt == "duration":
                    d = duration_days(rec.get("start"), rec.get("end"))
                    if fp in ("Q1","Q2","Q3"):
                        if not (form and form.startswith("10-Q") and d and Q_MIN<=d<=Q_MAX): continue
                    elif fp == "FY":
                        if not (form and form.startswith("10-K") and d and FY_MIN<=d<=FY_MAX): continue
                    elif fp == "Q4": continue  # derive via 10-K delta
                else:
                    if fp not in ("Q1","Q2","Q3","Q4","FY"): continue
                try: value = float(val) * m
                except Exception: continue
                yield {"cik":cik,"tag":tag,"unit":unit,"fy":int(fy),"fp":str(fp),"form":str(form),
                       "filed":rec.get("filed") or None,"start":rec.get("start"),"end":rec.get("end"),
                       "period_type":pt,"value":value}

def build_dim_security(FACTS: Path, DATA: Path, out_path: Path) -> pd.DataFrame:  # Light Python step
    rows = []
    it = os.scandir(FACTS)  # Faster directory walk than glob for many files
    i = 0
    for de in it:
        if not de.is_file() or not de.name.endswith(".json"): continue
        i += 1
        try:
            d = _json.loads(Path(de.path).read_bytes())
            cik = int(d.get("cik", 0))
            name = d.get("entityName") or ""
            ticker = d.get("ticker") or d.get("tickers")
            if isinstance(ticker, list) and ticker: ticker = str(ticker[0])
            rows.append({"cik":cik,"company_name":name,"ticker":ticker})
        except Exception: pass
        if i % 2000 == 0: print(f"dim_security progress {i}+")
    dim = pd.DataFrame(rows).drop_duplicates("cik")
    dim["cik"] = pd.to_numeric(dim["cik"], errors="coerce").astype("Int64")
    dim["ticker"] = dim["ticker"].astype(str).str.upper().str.strip()
    dim.loc[dim["ticker"].isin(["NONE","NAN","<NA>",""]), "ticker"] = pd.NA
    dim["company_name"] = (dim["company_name"].astype(str)
                           .str.replace(r"[^A-Za-z0-9 &\.-]+"," ",regex=True)
                           .str.replace(r"\s+"," ",regex=True)
                           .str.strip())
    map_csv = DATA / "ticker_map.csv"
    if map_csv.exists():
        tm = pd.read_csv(map_csv)
        tm.columns = [c.strip().lower() for c in tm.columns]
        if "cik" in tm.columns and "ticker" in tm.columns:
            tm["cik"] = pd.to_numeric(tm["cik"], errors="coerce").astype("Int64")
            tm["ticker"] = tm["ticker"].astype(str).str.upper().str.strip().replace("-", ".", regex=False)
            tm = tm.dropna(subset=["cik"]).drop_duplicates("cik", keep="first")
            dim = dim.merge(tm[["cik","ticker"]], on="cik", how="left", suffixes=("","_map"))
            dim["ticker"] = dim["ticker_map"].combine_first(dim["ticker"])
            dim.drop(columns=["ticker_map"], inplace=True, errors="ignore")
            print(f"✅ merged ticker_map.csv with {len(tm):,} rows")
        else: print("⚠️ ticker_map.csv missing 'cik' or 'ticker' — skipped merge")
    else: print("ℹ️ ticker_map.csv not found — dim_security uses embedded tickers only")
    dim.to_parquet(out_path)
    return dim

def main():  # Orchestrate streaming + DuckDB pipeline
    raw_parquet_path = MARTS / "_raw_us_gaap_usd.parquet"  # Streamed raw rows
    best_parquet_path = MARTS / "_best.parquet"  # De-duped rows
    for p in (raw_parquet_path, best_parquet_path):
        if p.exists(): p.unlink()  # Fresh intermediates

    arrow_schema = pa.schema([("cik",pa.int64()),("tag",pa.string()),("unit",pa.string()),("fy",pa.int32()),
                              ("fp",pa.string()),("form",pa.string()),("filed",pa.string()),
                              ("start",pa.string()),("end",pa.string()),("period_type",pa.string()),("value",pa.float64())])  # Fixed schema
    BUFFER_TARGET_ROWS = 15_000  # Lower buffer -> lower peak RAM (tune up/down)
    buffer = []  # Accumulating rows before flush
    writer = None  # ParquetWriter handle

    count = 0  # Files processed
    for de in os.scandir(FACTS):  # Stream directory entries
        if not de.is_file() or not de.name.endswith(".json"): continue
        count += 1
        try:
            cik, data = load_company_file(Path(de.path))  # Parse JSON
            for row in iter_all_us_gaap_usd(cik, data): buffer.append(row)  # Append normalized rows
            if len(buffer) >= BUFFER_TARGET_ROWS:  # Flush chunk
                tbl = pa.Table.from_pylist(buffer, schema=arrow_schema)
                if writer is None: writer = pq.ParquetWriter(raw_parquet_path, arrow_schema, compression="zstd")
                writer.write_table(tbl)
                buffer.clear()
                del tbl
                gc.collect()
        except Exception: pass  # Skip bad file and continue
        if count % 2000 == 0: print(f"parsed {count} companies")

    if buffer:  # Final flush
        tbl = pa.Table.from_pylist(buffer, schema=arrow_schema)
        if writer is None: writer = pq.ParquetWriter(raw_parquet_path, arrow_schema, compression="zstd")
        writer.write_table(tbl)
        buffer.clear()
        del tbl
    if writer is not None: writer.close()  # Close writer
    if not raw_parquet_path.exists(): raise RuntimeError("No USD us-gaap facts parsed (raw parquet not created).")
    print(f"✅ wrote {raw_parquet_path}")

    with duckdb.connect() as con:  # DuckDB session (auto-close)
        con.execute("PRAGMA threads=auto;")  # Use all cores
        con.execute(f"PRAGMA temp_directory='{str(TMP_DUCKDB).replace(\"'\",\"''\")}';")  # Spill to disk if needed
        con.execute("PRAGMA memory_limit='85%';")  # Allow DuckDB to use most memory but not all
        con.execute("CREATE OR REPLACE TEMP VIEW raw AS SELECT * FROM read_parquet(?);", [str(raw_parquet_path)])  # View: raw

        con.execute("""CREATE OR REPLACE TABLE best AS
                       WITH ranked AS (
                         SELECT *, 
                           CASE
                             WHEN period_type='duration' AND fp IN ('Q1','Q2','Q3') THEN CASE WHEN form LIKE '10-Q%%' THEN 0 ELSE 1 END
                             WHEN period_type='duration' AND fp='FY' THEN CASE WHEN form LIKE '10-K%%' THEN 0 ELSE 1 END
                             WHEN period_type='instant' AND fp IN ('Q1','Q2','Q3','Q4') THEN CASE WHEN form LIKE '10-Q%%' THEN 0 ELSE 1 END
                             WHEN period_type='instant' AND fp='FY' THEN CASE WHEN form LIKE '10-K%%' THEN 0 ELSE 1 END
                             ELSE 1
                           END AS form_rank,
                           TRY_CAST(filed AS DATE) AS filed_dt
                         FROM raw
                       ),
                       dedup AS (
                         SELECT *, ROW_NUMBER() OVER (
                           PARTITION BY cik, tag, fy, fp, period_type
                           ORDER BY form_rank ASC, filed_dt ASC
                         ) AS rn
                         FROM ranked
                       )
                       SELECT * FROM dedup WHERE rn=1;""")  # Rank + dedupe
        con.execute("COPY best TO ? (FORMAT PARQUET, COMPRESSION ZSTD);", [str(best_parquet_path)])
        print(f"✅ wrote {best_parquet_path}")

        fact_quarters_true_path = MARTS / "fact_quarters_true.parquet"  # Output paths
        fact_ttm_aligned_path = MARTS / "fact_ttm_aligned.parquet"
        dim_fye_path = MARTS / "dim_fiscal_year_end.parquet"
        latest_ttm_path = MARTS / "vw_latest_ttm.parquet"
        for p in (fact_quarters_true_path, fact_ttm_aligned_path, dim_fye_path, latest_ttm_path):
            if p.exists(): p.unlink()  # Clean old outputs

        con.execute("CREATE OR REPLACE VIEW best AS SELECT * FROM read_parquet(?);", [str(best_parquet_path)])  # View: best

        con.execute("""CREATE OR REPLACE TABLE quarters_true AS
                       WITH dur_q AS (
                         SELECT cik, tag, fy, fp,
                                CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 END AS qn,
                                value, period_type
                         FROM best
                         WHERE period_type='duration' AND fp IN ('Q1','Q2','Q3')
                       ),
                       dur_fy AS (
                         SELECT cik, tag, fy, value AS fy_value
                         FROM best
                         WHERE period_type='duration' AND fp='FY'
                       ),
                       dur_qsum AS (
                         SELECT cik, tag, fy, SUM(value) AS q123
                         FROM dur_q
                         GROUP BY 1,2,3
                       ),
                       dur_q4_delta AS (
                         SELECT f.cik, f.tag, f.fy, '10K delta' AS fp, 4 AS qn,
                                (f.fy_value - q.q123) AS value, 'duration' AS period_type
                         FROM dur_fy f JOIN dur_qsum q USING(cik, tag, fy)
                       ),
                       ins_q AS (
                         SELECT cik, tag, fy, fp,
                                CASE fp WHEN 'Q1' THEN 1 WHEN 'Q2' THEN 2 WHEN 'Q3' THEN 3 WHEN 'Q4' THEN 4 END AS qn,
                                value, period_type
                         FROM best
                         WHERE period_type='instant' AND fp IN ('Q1','Q2','Q3','Q4')
                       ),
                       ins_fy AS (
                         SELECT cik, tag, fy, value
                         FROM best
                         WHERE period_type='instant' AND fp='FY'
                       ),
                       have_q4 AS (
                         SELECT DISTINCT cik, tag, fy FROM ins_q WHERE fp='Q4'
                       ),
                       ins_fy_to_q4 AS (
                         SELECT f.cik, f.tag, f.fy, 'Q4' AS fp, 4 AS qn, f.value, 'instant' AS period_type
                         FROM ins_fy f LEFT JOIN have_q4 h USING(cik, tag, fy)
                         WHERE h.cik IS NULL
                       )
                       SELECT * FROM dur_q
                       UNION ALL SELECT * FROM dur_q4_delta
                       UNION ALL SELECT * FROM ins_q
                       UNION ALL SELECT * FROM ins_fy_to_q4;""")  # Build true quarters
        con.execute("COPY quarters_true TO ? (FORMAT PARQUET, COMPRESSION ZSTD);", [str(fact_quarters_true_path)])
        print(f"✅ wrote {fact_quarters_true_path}")

        con.execute("""CREATE OR REPLACE TABLE ttm AS
                       WITH base AS (SELECT cik, tag, fy, fp, qn, value, period_type FROM quarters_true),
                       w AS (
                         SELECT *,
                           CASE WHEN period_type='duration'
                                THEN SUM(value) OVER (PARTITION BY cik, tag, period_type ORDER BY fy, qn ROWS BETWEEN 3 PRECEDING AND CURRENT ROW)
                                ELSE value END AS TTM_value,
                           CASE WHEN period_type='duration'
                                THEN COUNT(value) OVER (PARTITION BY cik, tag, period_type ORDER BY fy, qn ROWS BETWEEN 3 PRECEDING AND CURRENT ROW)
                                ELSE 1 END AS cnt4
                         FROM base
                       )
                       SELECT cik, tag, fy, fp, qn, value, period_type, TTM_value
                       FROM w
                       WHERE (period_type='instant' OR cnt4=4) AND TTM_value IS NOT NULL;""")  # TTM with rolling 4Q

        con.execute("""CREATE OR REPLACE TABLE dim_fiscal_year_end AS
                       WITH fy AS (
                         SELECT cik, TRY_CAST(end AS DATE) AS end_dt
                         FROM best
                         WHERE period_type='duration' AND fp='FY' AND end IS NOT NULL
                       ),
                       months AS (
                         SELECT cik, EXTRACT(MONTH FROM end_dt)::INT AS fye_month, COUNT(*) AS cnt
                         FROM fy
                         WHERE end_dt IS NOT NULL
                         GROUP BY 1,2
                       ),
                       ranked AS (
                         SELECT *, ROW_NUMBER() OVER (PARTITION BY cik ORDER BY cnt DESC, fye_month DESC) AS rn
                         FROM months
                       )
                       SELECT cik, fye_month, ((fye_month % 12)+1)::INT AS fiscal_year_start_month
                       FROM ranked
                       WHERE rn=1;""")  # Infer FYE month by mode
        con.execute("COPY dim_fiscal_year_end TO ? (FORMAT PARQUET, COMPRESSION ZSTD);", [str(dim_fye_path)])
        print(f"✅ wrote {dim_fye_path}")

        con.execute("""CREATE OR REPLACE TABLE ttm_aligned AS
                       WITH a AS (SELECT t.*, d.fye_month FROM ttm t LEFT JOIN dim_fiscal_year_end d USING(cik)),
                       aln AS (
                         SELECT a.*,
                                (((fye_month + CASE qn WHEN 4 THEN 0 WHEN 3 THEN -3 WHEN 2 THEN -6 WHEN 1 THEN -9 END) - 1) % 12 + 1) AS quarter_end_month
                         FROM a
                         WHERE fye_month IS NOT NULL AND qn IS NOT NULL
                       )
                       SELECT *,
                              ((quarter_end_month - 1)/3 + 1)::INT AS calendar_quarter,
                              CASE WHEN quarter_end_month <= fye_month THEN fy ELSE fy - 1 END AS calendar_year
                       FROM aln;""")  # Align to calendar
        con.execute("COPY ttm_aligned TO ? (FORMAT PARQUET, COMPRESSION ZSTD);", [str(fact_ttm_aligned_path)])
        print(f"✅ wrote {fact_ttm_aligned_path}")

    dim_sec = build_dim_security(FACTS, DATA, MARTS / "dim_security.parquet")  # Build dim_security (light)
    print(f"✅ wrote {MARTS / 'dim_security.parquet'}  rows={len(dim_sec):,}")  # Log count

    with duckdb.connect() as con:  # Final latest view in DuckDB
        con.execute("PRAGMA threads=auto;")
        con.execute(f"PRAGMA temp_directory='{str(TMP_DUCKDB).replace(\"'\",\"''\")}';")
        con.execute("CREATE OR REPLACE VIEW ttm_aligned AS SELECT * FROM read_parquet(?);", [str(fact_ttm_aligned_path)])
        con.execute("CREATE OR REPLACE VIEW dim_security AS SELECT * FROM read_parquet(?);", [str(MARTS / "dim_security.parquet")])
        con.execute("""CREATE OR REPLACE TABLE vw_latest_ttm AS
                       WITH a AS (
                         SELECT cik, tag, period_type, calendar_year, calendar_quarter, TTM_value
                         FROM ttm_aligned
                         WHERE calendar_year IS NOT NULL AND calendar_quarter IS NOT NULL
                       ),
                       ranked AS (
                         SELECT a.*, ROW_NUMBER() OVER (
                           PARTITION BY cik, tag ORDER BY calendar_year DESC, calendar_quarter DESC
                         ) AS rn
                         FROM a
                       )
                       SELECT r.cik, s.ticker, s.company_name, r.tag, r.period_type,
                              r.calendar_year AS ttm_cal_year, r.calendar_quarter AS ttm_cal_quarter,
                              r.TTM_value AS ttm_value
                       FROM ranked r LEFT JOIN dim_security s USING(cik)
                       WHERE rn=1
                       ORDER BY ttm_value DESC NULLS LAST;""")  # Latest per (cik,tag)
        con.execute("COPY vw_latest_ttm TO ? (FORMAT PARQUET, COMPRESSION ZSTD);", [str(latest_ttm_path)])
        print(f"✅ wrote {latest_ttm_path}")

    try:
        vw = pd.read_parquet(MARTS / "vw_latest_ttm.parquet")  # Optional preview
        print("\nTop 10 latest TTM (any tag):")
        print(vw.head(10))
    except Exception as e:
        print(f"Note: could not preview vw_latest_ttm due to: {e}")

if __name__ == "__main__": main()  # Run
